# ðŸ“š Complete Technical Deep Dive & Interview Prep Guide

---

## ðŸŽ¯ PART 1: TECHNICAL CONCEPTS EXPLAINED IN DEPTH

---

### **1. DISTRIBUTED MICROSERVICES ARCHITECTURE**

**What it is:**
Breaking a monolithic application into small, independent services where each service handles a specific business capability and runs in its own process.

**In your project:**
- **8 services**: API Gateway, Socket Gateway, Worker Enricher, Feed Generator, Storage, Auth, Analytics, Logging
- Each service has its own codebase, can be deployed independently, and communicates via HTTP REST APIs or Redis Pub/Sub

**Why it matters:**
- **Scalability**: Can scale individual services based on load (e.g., scale workers without scaling API gateway)
- **Fault Isolation**: If one service crashes, others continue working
- **Technology Flexibility**: Each service can use different tech stack
- **Team Autonomy**: Different teams can own different services

**Key characteristics in your code:**
```python
# Each service runs independently on different ports
API Gateway: 8000
Auth: 8001
Socket Gateway: 8002
Storage: 8003
Analytics: 8004
Logging: 8005
```

---

### **2. EVENT-DRIVEN ARCHITECTURE**

**What it is:**
Services communicate by publishing and subscribing to events rather than direct API calls. When something happens, an event is published, and interested services react to it.

**In your project:**
```python
# Feed Generator publishes events
redis_client.publish('market:underlying', json.dumps(tick_data))
redis_client.publish('market:option_chain', json.dumps(chain_data))

# Worker Enricher subscribes to events
pubsub.subscribe('market:underlying', 'market:option_quote', 'market:option_chain')
```

**Benefits:**
- **Loose Coupling**: Services don't need to know about each other
- **Asynchronous**: Publisher doesn't wait for subscribers
- **Multiple Consumers**: Many services can react to same event
- **Resilience**: If a subscriber is down, events aren't lost (Redis handles buffering)

**5 Channels in your project:**
1. `market:underlying` - Raw underlying price ticks
2. `market:option_quote` - Individual option quotes
3. `market:option_chain` - Complete option chains
4. `enriched:underlying` - Processed underlying data
5. `enriched:option_chain` - Enriched chain with analytics

---

### **3. REDIS PUB/SUB**

**What it is:**
Redis Publish/Subscribe is a messaging pattern where publishers send messages to channels without knowing who will receive them, and subscribers listen to channels of interest.

**How it works:**
```
Publisher â†’ Redis Channel â†’ Multiple Subscribers
(Fire and forget)      (Message broker)     (All receive message)
```

**In your code:**
```python
# Publisher (Feed Generator)
redis_client.publish('market:underlying', json.dumps({
    'product': 'NIFTY',
    'price': 21543.25,
    'timestamp': '2025-01-15T10:30:00Z'
}))

# Subscriber (Worker Enricher)
pubsub = redis_client.pubsub()
pubsub.subscribe('market:underlying')
for message in pubsub.listen():
    if message['type'] == 'message':
        data = json.loads(message['data'])
        process_underlying_tick.delay(data)  # Dispatch to Celery
```

**Key Differences from Queue:**
- **Pub/Sub**: Message delivered to ALL subscribers (1-to-many)
- **Queue**: Message delivered to ONE consumer (1-to-1)

---

### **4. CELERY - DISTRIBUTED TASK QUEUE**

**What it is:**
Celery is a distributed task queue system that allows you to run functions asynchronously in background workers.

**Architecture:**
```
Producer â†’ Broker (Redis) â†’ Worker â†’ Result Backend (Redis)
(Your app)  (Queue)        (Celery)   (Stores results)
```

**In your code:**
```python
# Define task
@celery_app.task
def process_option_chain(chain_data):
    # Heavy computation here
    calculate_pcr()
    calculate_max_pain()
    store_in_mongodb()

# Dispatch task (non-blocking)
process_option_chain.delay(chain_data)  # Returns immediately
```

**Why use Celery:**
- Offload heavy computations from API requests
- Retry failed tasks automatically
- Scale workers independently
- Handle long-running tasks without blocking

**Your Celery configuration:**
```python
celery_app.conf.update(
    task_serializer='json',
    task_acks_late=True,              # Critical for reliability
    worker_prefetch_multiplier=1,     # One task at a time per worker
    task_reject_on_worker_lost=True,  # Re-queue if worker crashes
)
```

---

### **5. EXPONENTIAL BACKOFF RETRY**

**What it is:**
When a task fails, retry it with increasing delays: 5s â†’ 10s â†’ 20s â†’ 40s

**Why exponential:**
- Prevents overwhelming failing service with retries
- Gives time for transient issues to resolve (network hiccup, DB lock)
- Reduces thundering herd problem

**In your code:**
```python
celery_app.conf.update(
    task_autoretry_for=(Exception,),           # Retry on any exception
    task_retry_kwargs={'max_retries': 3, 'countdown': 5},  # Max 3, start at 5s
    task_default_retry_delay=5,
)
```

**Retry timeline:**
```
Attempt 1: Fails â†’ Wait 5s
Attempt 2: Fails â†’ Wait 10s (2^1 * 5s)
Attempt 3: Fails â†’ Wait 20s (2^2 * 5s)
Attempt 4: Fails â†’ Send to DLQ
```

**Better than fixed retry:**
- Fixed: Retry every 5s â†’ Can overwhelm system
- Exponential: Increasing delays â†’ Gives system breathing room

---

### **6. DEAD LETTER QUEUE (DLQ)**

**What it is:**
A special queue where messages that failed after all retry attempts are stored for later inspection/manual handling.

**In your code:**
```python
def on_failure(self, exc, task_id, args, kwargs, einfo):
    # After max retries exhausted, send to DLQ
    dlq_message = {
        'task_id': task_id,
        'task_name': self.name,
        'error': str(exc),
        'args': args,
        'timestamp': datetime.now().isoformat()
    }
    redis_client.lpush('dlq:enrichment', json.dumps(dlq_message))
```

**Why DLQ is critical:**
- **No data loss**: Failed messages aren't discarded
- **Debugging**: Inspect what went wrong
- **Replay capability**: Can manually re-process later
- **Alerting**: Monitor DLQ size to detect issues

**Viewing DLQ:**
```bash
redis-cli LRANGE dlq:enrichment 0 -1
```

---

### **7. TASK IDEMPOTENCY**

**What it is:**
Ensuring a task can be executed multiple times without changing the result beyond the first execution.

**Why it matters:**
- Network issues can cause duplicate messages
- Retries can process same task twice
- Worker crashes can lead to re-processing

**In your code:**
```python
def process_underlying_tick(tick_data):
    tick_id = tick_data['tick_id']
    product = tick_data['product']
    
    # Check if already processed
    idempotency_key = f"processed:underlying:{product}:{tick_id}"
    if redis_client.exists(idempotency_key):
        return  # Already processed, skip
    
    # Process the tick
    store_in_mongodb(tick_data)
    update_cache(tick_data)
    
    # Mark as processed (TTL 1 hour)
    redis_client.setex(idempotency_key, 3600, '1')
```

**Without idempotency:**
- Same tick processed twice â†’ Duplicate DB entries
- Analytics calculated multiple times â†’ Incorrect metrics

---

### **8. LATE ACKNOWLEDGMENT PATTERN (acks_late=True)**

**What it is:**
Task is acknowledged to broker only AFTER successful completion, not when worker receives it.

**Two modes:**
```
Early Ack (default):
Worker receives task â†’ Send ACK â†’ Process task â†’ Done
Problem: If worker crashes during processing, task is lost

Late Ack (acks_late=True):
Worker receives task â†’ Process task â†’ Send ACK â†’ Done
Benefit: If worker crashes, task is NOT acked, so broker re-queues it
```

**In your code:**
```python
celery_app.conf.update(
    task_acks_late=True,  # Don't ack until task completes
)
```

**Why it matters:**
- Prevents message loss on worker crash
- Ensures at-least-once delivery
- Combined with idempotency = exactly-once processing

---

### **9. REDIS CACHING - CACHE-ASIDE PATTERN**

**What it is:**
Application code is responsible for reading from cache and updating it.

**Flow:**
```
1. Request arrives
2. Check cache (Redis)
   â”œâ”€ Cache HIT â†’ Return cached data
   â””â”€ Cache MISS â†’ Query database â†’ Store in cache â†’ Return data
```

**In your code:**
```python
def get_underlying(product):
    # Try cache first
    cache_key = f"latest:underlying:{product}"
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)  # Cache HIT
    
    # Cache MISS - query database
    data = db.underlying_ticks.find_one({'product': product})
    
    # Update cache with TTL
    redis_client.setex(cache_key, 300, json.dumps(data))  # 5 min TTL
    
    return data
```

**Alternative patterns:**
- **Write-through**: Write to cache + DB simultaneously
- **Write-behind**: Write to cache, async write to DB later

**Why cache-aside:**
- Application controls what/when to cache
- Read-heavy workloads (your use case)
- DB is source of truth

---

### **10. TTL (TIME TO LIVE)**

**What it is:**
Automatic expiration of cached data after specified time.

**In your code:**
```python
# Cache with 5-minute TTL
redis_client.setex(f"latest:underlying:{product}", 300, json.dumps(data))
                                                    ^^^
                                                  300 seconds = 5 min
```

**Why TTL:**
- **Memory management**: Old data auto-deleted
- **Freshness**: Forces re-fetch of stale data
- **No manual invalidation**: Redis handles cleanup

**TTL strategy in your project:**
```
Hot data (latest prices): 5 min TTL
OHLC windows: Window duration TTL (5min window = 5min TTL)
Idempotency keys: 1 hour TTL
```

---

### **11. MULTI-LEVEL CACHING (3 CACHE LEVELS)**

**What it is:**
Multiple cache layers with different data and TTLs.

**Your 3 levels:**

**Level 1: Latest Data (Real-time, 5min TTL)**
```python
latest:underlying:{product}      # Latest price
latest:option:{symbol}           # Latest quote
latest:chain:{product}:{expiry}  # Latest chain
```

**Level 2: Computed Data (Analytics, varies)**
```python
ohlc:{product}:{window}m         # OHLC windows
volatility_surface:{product}     # IV surface
```

**Level 3: Operational Data (1hr TTL)**
```python
processed:underlying:{product}:{tick_id}  # Idempotency tracking
```

**Why multi-level:**
- Different data has different freshness requirements
- Separate hot path from cold path
- Optimize memory usage

---

### **12. REDIS SORTED SETS**

**What it is:**
Redis data structure where each member has a score, automatically sorted by score.

**In your code:**
```python
# Store IV surface data (score = strike price)
redis_client.zadd(
    f"iv_surface:{product}",
    {
        json.dumps({'strike': 21500, 'iv': 0.25, 'expiry': '2025-01-25'}): 21500
    }
)

# Query by strike range
redis_client.zrangebyscore(f"iv_surface:{product}", 21000, 22000)
```

**Use cases:**
- Leaderboards (score = points)
- Time-series (score = timestamp)
- Volatility surfaces (score = strike price)

**Why sorted sets for IV:**
- Efficient range queries (strikes between X and Y)
- Automatically sorted by strike
- O(log N) insert/lookup

---

### **13. WEBSOCKET (Flask-SocketIO)**

**What it is:**
Full-duplex communication protocol allowing bi-directional real-time data between client and server over a single TCP connection.

**HTTP vs WebSocket:**
```
HTTP (Request-Response):
Client â†’ Request â†’ Server
Client â† Response â† Server
(Need new request for updates)

WebSocket (Persistent Connection):
Client â†â†’ Server
(Server can push updates anytime)
```

**In your code:**
```python
# Server pushes data to client
socketio.emit('underlying_update', data, room='product:NIFTY')

# Client receives instantly (no polling needed)
socket.on('underlying_update', (data) => {
    console.log('Live price:', data.price);
});
```

**Why WebSocket for trading:**
- Real-time price updates (no polling)
- Low latency (<100ms)
- Efficient (single connection vs repeated HTTP requests)

---

### **14. ROOM-BASED SUBSCRIPTIONS**

**What it is:**
Socket.IO feature allowing clients to join "rooms" and receive targeted broadcasts.

**In your code:**
```python
# Client joins room
@socketio.on('subscribe')
def handle_subscribe(data):
    symbol = data['symbol']
    room = f"product:{symbol}"
    join_room(room)  # Client now in 'product:NIFTY' room

# Server broadcasts to specific room
socketio.emit('underlying_update', data, room='product:NIFTY')
# Only clients in 'product:NIFTY' room receive this
```

**Your room structure:**
```
'general' â†’ All clients (auto-joined)
'product:NIFTY' â†’ Only NIFTY subscribers
'product:BANKNIFTY' â†’ Only BANKNIFTY subscribers
'chain:NIFTY' â†’ NIFTY option chain subscribers
```

**Benefits:**
- **Bandwidth efficiency**: Clients receive only what they subscribed to
- **Scalability**: Don't broadcast everything to everyone
- **Flexibility**: Clients control what data they receive

---

### **15. REDIS MESSAGE QUEUE (for WebSocket)**

**What it is:**
Using Redis to coordinate multiple Socket.IO server instances so they can share connections.

**Problem without it:**
```
Client connects to Server 1
Data arrives at Server 2
Server 2 can't push to client (connected to Server 1)
```

**Solution with Redis message queue:**
```python
socketio = SocketIO(
    app,
    message_queue=redis_url  # All servers share via Redis
)

# Server 2 emits
socketio.emit('update', data, room='product:NIFTY')
â†“
Redis broadcasts to all Socket.IO servers
â†“
Server 1 receives and pushes to its connected clients
```

**Enables:**
- **Horizontal scaling**: Run multiple Socket.IO instances
- **Load balancing**: Distribute clients across servers
- **High availability**: If one server dies, others continue

---

### **16. MULTI-INSTANCE COORDINATION**

**What it is:**
Multiple instances of same service running simultaneously, coordinated through shared state (Redis).

**In your project:**
```
Socket Gateway Instance 1 â†â†’ Redis â†â†’ Socket Gateway Instance 2
      (Clients A, B)                        (Clients C, D)
```

**Coordination mechanisms:**
1. **Redis message queue**: Share WebSocket events
2. **Redis pub/sub**: Share data updates
3. **Redis cache**: Share state (connected clients, room memberships)

**Why coordinate:**
- Load balancing across instances
- No single point of failure
- Session persistence (client can reconnect to any instance)

---

### **17. KUBERNETES HPA (Horizontal Pod Autoscaler)**

**What it is:**
Kubernetes feature that automatically scales number of pods based on CPU/memory usage.

**In your manifest:**
```yaml
spec:
  minReplicas: 2        # Never fewer than 2
  maxReplicas: 10       # Never more than 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 70  # Scale up if CPU > 70%
```

**How it works:**
```
Normal load: 2 replicas running
â†“
Load increases, CPU hits 75%
â†“
HPA creates 1 more replica (now 3 total)
â†“
Load increases further, CPU still high
â†“
HPA creates more replicas (up to 10 max)
â†“
Load decreases, CPU drops below 70%
â†“
HPA removes replicas (down to 2 min)
```

**Why HPA:**
- **Cost efficiency**: Pay only for what you need
- **Automatic**: No manual intervention
- **Handles spikes**: Black swan events, market crashes â†’ auto-scale

---

### **18. DOCKER CONTAINERIZATION**

**What it is:**
Packaging application with all dependencies into isolated, portable containers.

**Your Dockerfile structure:**
```dockerfile
FROM python:3.10
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
```

**Benefits:**
- **Consistency**: Same environment dev â†’ staging â†’ prod
- **Isolation**: Each service in own container
- **Portability**: Runs anywhere (local, AWS, GCP, Azure)
- **Microservices**: Each service = separate container

---

### **19. MONGODB COMPOUND INDEXES**

**What it is:**
Index on multiple fields to optimize queries filtering/sorting by those fields.

**In your code:**
```python
# Compound index on (product + timestamp)
db.underlying_ticks.create_index([
    ('product', ASCENDING),
    ('timestamp', DESCENDING)
])
```

**Query optimization:**
```python
# This query uses the compound index efficiently
db.underlying_ticks.find({
    'product': 'NIFTY',
    'timestamp': {'$gte': start_time}
}).sort('timestamp', -1)

# Without index: Full collection scan (slow)
# With index: Index seek (fast)
```

**Your 3 indexed collections:**
1. `underlying_ticks`: (product, timestamp)
2. `option_quotes`: (product, timestamp) and (symbol, timestamp)
3. `option_chains`: (product, expiry, timestamp)

**Why compound over single:**
- Single index on `product`: Can filter by product, but sort is slow
- Compound index on `(product, timestamp)`: Fast filter + fast sort

---

### **20. STRUCTURED JSON LOGGING**

**What it is:**
Logging in JSON format with structured fields rather than plain text.

**Traditional logging:**
```
ERROR - Failed to process tick for NIFTY at 2025-01-15 10:30:00
```

**Structured JSON logging:**
```json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "error",
  "service": "worker-enricher",
  "event": "tick_processing_failed",
  "product": "NIFTY",
  "tick_id": 12345,
  "error": "MongoDB connection timeout",
  "trace_id": "abc123"
}
```

**In your code:**
```python
import structlog

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
logger = structlog.get_logger()

logger.error(
    "tick_processing_failed",
    product="NIFTY",
    tick_id=12345,
    error=str(e)
)
```

**Benefits:**
- **Searchable**: Query logs by any field (show me all NIFTY errors)
- **Aggregatable**: Count errors by product, service, etc.
- **Machine-readable**: Easy to parse by log aggregators (ELK, Splunk)
- **Contextual**: Include request ID, user ID, trace ID

---

### **21. PROMETHEUS METRICS**

**What it is:**
Time-series monitoring system that scrapes metrics from services.

**Typical metrics in your services:**
```python
# Counter (always increasing)
requests_total{service="api-gateway", endpoint="/api/data/products"} 1523

# Gauge (can go up/down)
connected_websocket_clients{service="socket-gateway"} 47

# Histogram (distributions)
request_duration_seconds{service="storage"} 0.025
```

**In your code:**
```python
@app.route('/metrics')
def metrics():
    return {
        'total_clients': len(connected_clients),
        'rooms': room_counts,
        'messages_sent': message_counter
    }
```

**Why Prometheus:**
- Real-time monitoring
- Alerting (PagerDuty if CPU > 80%)
- Grafana dashboards
- Trend analysis

---

### **22. OBSERVABILITY STACK**

**What it is:**
Combination of logging, metrics, and tracing to understand system behavior.

**Three pillars:**

**1. Logs (Structured JSON)**
- What happened: "Order processed"
- When: "2025-01-15T10:30:00Z"
- Context: product, user_id, error details

**2. Metrics (Prometheus)**
- How many: Requests per second
- How fast: Latency percentiles (p50, p95, p99)
- How much: CPU, memory, disk usage

**3. Traces (Optional - Jaeger/Zipkin)**
- Request flow across services
- Latency breakdown per service
- Bottleneck identification

**Your observability:**
- Structured logging (structlog)
- Metrics endpoints (/health, /metrics)
- Health checks (MongoDB ping, Redis ping)

---

### **23. PCR (PUT-CALL RATIO)**

**What it is:**
Trading metric comparing put option volume/OI to call option volume/OI.

**Calculation:**
```python
total_call_oi = sum(c['open_interest'] for c in calls)
total_put_oi = sum(p['open_interest'] for p in puts)
pcr = total_put_oi / total_call_oi
```

**Interpretation:**
- PCR > 1.0: More puts than calls (bearish sentiment)
- PCR < 1.0: More calls than puts (bullish sentiment)
- PCR = 0.7-1.0: Typical neutral range

**In trading:**
- Contrarian indicator (high PCR = possible reversal)
- Sentiment gauge (institutional vs retail)

---

### **24. MAX PAIN**

**What it is:**
Strike price where option writers (sellers) profit the most, and option buyers lose the most.

**Calculation:**
```python
def calculate_max_pain(calls, puts, strikes):
    max_pain = None
    min_total_value = float('inf')
    
    for strike in strikes:
        # Total value of all options at this strike
        call_value = sum(
            c['open_interest'] * max(0, strike - c['strike'])
            for c in calls
        )
        put_value = sum(
            p['open_interest'] * max(0, p['strike'] - strike)
            for p in puts
        )
        total_value = call_value + put_value
        
        if total_value < min_total_value:
            min_total_value = total_value
            max_pain = strike
    
    return max_pain
```

**Theory:**
- Market tends to gravitate toward max pain at expiry
- Option writers (market makers) hedge to push price there
- Useful for predicting expiry settlement

---

### **25. IMPLIED VOLATILITY (IV) SURFACE**

**What it is:**
3D representation of implied volatility across strikes and expiries.

**Dimensions:**
- X-axis: Strike price
- Y-axis: Time to expiry
- Z-axis: Implied volatility

**In your code:**
```python
surface = {
    'product': 'NIFTY',
    'expiries': [
        {
            'expiry': '2025-01-25',
            'strikes': [21000, 21500, 22000],
            'ivs': [0.25, 0.20, 0.23],  # Higher IV at extremes (smile)
        },
        {
            'expiry': '2025-02-28',
            'strikes': [21000, 21500, 22000],
            'ivs': [0.22, 0.18, 0.20],
        }
    ]
}
```

**Why it matters:**
- **Volatility smile**: OTM options have higher IV
- **Volatility skew**: Puts have higher IV than calls (fear premium)
- **Arbitrage**: Identify mispriced options
- **Risk management**: Understand portfolio volatility exposure

---

### **26. OPTION GREEKS**

**What they are:**
Measures of option price sensitivity to various factors.

**Delta:**
- Change in option price per $1 change in underlying
- Call delta: 0 to 1, Put delta: -1 to 0
- Delta = 0.5 means ATM (50% chance of expiring ITM)

**Gamma:**
- Change in delta per $1 change in underlying
- Highest at ATM
- Shows how delta changes (acceleration)

**Vega:**
- Change in option price per 1% change in IV
- Long options have positive vega (benefit from volatility increase)

**Theta:**
- Option value lost per day (time decay)
- Always negative for long options
- Accelerates near expiry

---

### **27. OHLC WINDOWS**

**What it is:**
Aggregating tick data into time-based candles.

**In your code:**
```python
def calculate_ohlc_window(product, window_minutes):
    # Get ticks from last N minutes
    start_time = datetime.now() - timedelta(minutes=window_minutes)
    ticks = db.underlying_ticks.find({
        'product': product,
        'timestamp': {'$gte': start_time}
    }).sort('timestamp', ASCENDING)
    
    prices = [t['price'] for t in ticks]
    ohlc = {
        'open': prices[0],      # First price
        'high': max(prices),    # Highest price
        'low': min(prices),     # Lowest price
        'close': prices[-1],    # Last price
    }
    return ohlc
```

**Your windows:**
- 1-minute: High-frequency trading, scalping
- 5-minute: Intraday trading, technical analysis
- 15-minute: Swing trading, trend identification

---

## ðŸŽ¯ PART 2: COMPREHENSIVE INTERVIEW QUESTIONS & ANSWERS

---

## ðŸ“š **SECTION 1: MICROSERVICES ARCHITECTURE**

---

### **Q1: Explain your microservices architecture. Why did you choose this design?**

**Answer:**
"We built an 8-service architecture for a real-time trading analytics platform. Let me walk through the design:

**Services:**
1. **API Gateway** (Port 8000): Single entry point, routes requests to appropriate services
2. **Auth Service** (Port 8001): JWT-based authentication, user management
3. **Socket Gateway** (Port 8002): WebSocket server for real-time updates
4. **Storage Service** (Port 8003): MongoDB abstraction layer, data access
5. **Analytics Service** (Port 8004): PCR, Max Pain, IV surface calculations
6. **Worker Enricher**: Celery workers processing market data
7. **Feed Generator**: Simulates market data feeds
8. **Logging Service** (Port 8005): Centralized logging

**Why microservices:**
- **Independent scaling**: Workers process heavy analytics, so we can scale them (2-10 replicas) without scaling the API Gateway
- **Technology flexibility**: Could use Python for workers but Node.js for WebSocket if needed
- **Fault isolation**: If Analytics service crashes, real-time data streaming continues
- **Team autonomy**: Different developers can own different services
- **Deployment**: Can deploy Analytics changes without touching Auth

**Communication:**
- Synchronous: HTTP REST between API Gateway and other services
- Asynchronous: Redis Pub/Sub for event-driven data flow

This design allowed us to process real-time market data with <100ms latency while maintaining 99%+ uptime."

**Follow-up Q: What challenges did you face with microservices?**

"Three main challenges:

1. **Distributed debugging**: When a request fails, need to trace across 3-4 services. Solution: Implemented structured JSON logging with trace IDs

2. **Data consistency**: No ACID transactions across services. Solution: Used event sourcing with idempotency to achieve eventual consistency

3. **Service discovery**: Services need to find each other. Solution: Used Docker Compose networking (service names as DNS) for local, would use Kubernetes services in production

4. **Network latency**: Inter-service calls add latency. Solution: Cached frequently accessed data in Redis with 5min TTL"

---

### **Q2: How do services communicate? Walk me through a request flow.**

**Answer:**
"We use two communication patterns:

**Pattern 1: Synchronous (REST API)**
Used when client needs immediate response.

Example: User requests NIFTY option chain
```
Client â†’ API Gateway (8000) â†’ Storage Service (8003) â†’ MongoDB
       â† JSON Response  â†
```

**Pattern 2: Asynchronous (Redis Pub/Sub)**
Used for real-time data flow.

Example: Market data processing
```
Feed Generator â†’ Redis Pub/Sub â†’ Worker Enricher â†’ MongoDB + Redis Cache
                 (market:underlying)    (Celery)     (Storage + Cache)
                                            â†“
                                    Redis Pub/Sub â†’ Socket Gateway â†’ WebSocket Clients
                                 (enriched:underlying)   (8002)
```

**Detailed flow for option chain:**
1. Feed Generator publishes to `market:option_chain` channel
2. Worker Enricher subscriber receives message
3. Dispatches Celery task `process_option_chain.delay(data)`
4. Worker:
   - Calculates PCR, Max Pain, Greeks
   - Stores in MongoDB
   - Updates Redis cache with 5min TTL
   - Publishes to `enriched:option_chain`
5. Socket Gateway receives enriched data
6. Broadcasts to subscribed WebSocket clients in real-time

Total latency: ~50-100ms from feed to client"

**Follow-up Q: Why use both patterns? Why not just REST everywhere?**

"Great question. Each has its use case:

**REST (Synchronous):**
- âœ… Client needs immediate response (API requests)
- âœ… Strong consistency required
- âœ… Simple request-response
- âŒ Blocks caller, coupling services

**Pub/Sub (Asynchronous):**
- âœ… Fire-and-forget (no response needed)
- âœ… Multiple consumers (Worker, Socket Gateway both listen)
- âœ… Loose coupling (publisher doesn't know subscribers)
- âœ… High throughput (non-blocking)
- âŒ No immediate response
- âŒ Eventual consistency

For our trading platform:
- User queries (Get option chain) â†’ REST (need data immediately)
- Market data processing â†’ Pub/Sub (high throughput, multiple consumers)

This hybrid approach gives us best of both worlds."

---

### **Q3: What happens if one service goes down?**

**Answer:**
"We designed for fault tolerance at multiple levels:

**Scenario 1: Worker Enricher crashes**
- Redis Pub/Sub buffers messages (up to memory limit)
- When worker restarts, catches up on backlog
- Celery's `acks_late=True` ensures no message loss
- DLQ catches poison messages

**Scenario 2: Storage Service crashes**
- API Gateway returns 503 Service Unavailable
- Circuit breaker pattern (could implement) prevents cascade failure
- MongoDB replica set ensures data persistence
- Recent data still available from Redis cache

**Scenario 3: Socket Gateway crashes**
- Clients automatically reconnect (Socket.IO built-in)
- Redis message queue allows other Socket Gateway instances to serve clients
- No data loss (data in Redis/MongoDB)

**Scenario 4: Redis crashes**
- Biggest impact: Cache miss on all requests â†’ Higher DB load
- Pub/Sub messages lost (not persistent by default)
- Solution: Redis Sentinel/Cluster for HA, or Kafka for persistent messaging

**Mitigation strategies:**
1. **Health checks**: Each service exposes `/health` endpoint
2. **Kubernetes liveness probes**: Auto-restart unhealthy pods
3. **HPA**: Scale up healthy instances when others fail
4. **Retry logic**: Services retry failed requests with exponential backoff
5. **Graceful degradation**: Serve cached data if DB unavailable"

**Follow-up Q: How would you improve fault tolerance?**

"Three improvements:

1. **Circuit Breaker Pattern**: If Storage Service fails 5 times in 10s, API Gateway stops calling it for 30s, returns cached data

2. **Message persistence**: Replace Redis Pub/Sub with Kafka for durable messages (survives crashes)

3. **Database replication**: MongoDB replica set with read replicas (1 primary, 2 secondary) for high availability

4. **Rate limiting**: Prevent cascading failures from one overloaded service affecting others"

---

## ðŸ“š **SECTION 2: REDIS PUB/SUB & EVENT-DRIVEN ARCHITECTURE**

---

### **Q4: Explain how Redis Pub/Sub works in your system.**

**Answer:**
"Redis Pub/Sub is our message broker for event-driven data flow. Let me explain:

**Core Concept:**
Publishers send messages to channels. Subscribers listen to channels. Redis routes messages from publishers to all subscribers of that channel.

**Our implementation:**

**Publishers:**
- Feed Generator publishes market data
```python
redis_client.publish('market:underlying', json.dumps({
    'product': 'NIFTY',
    'price': 21543.25,
    'timestamp': '2025-01-15T10:30:00Z'
}))
```

**Subscribers:**
- Worker Enricher subscribes to process data
```python
pubsub = redis_client.pubsub()
pubsub.subscribe('market:underlying', 'market:option_chain')

for message in pubsub.listen():
    if message['type'] == 'message':
        data = json.loads(message['data'])
        if message['channel'] == 'market:underlying':
            process_underlying_tick.delay(data)
```

**Our 5 channels:**
1. `market:underlying` - Raw price ticks
2. `market:option_quote` - Individual option quotes
3. `market:option_chain` - Complete option chains
4. `enriched:underlying` - Processed with OHLC, cached
5. `enriched:option_chain` - With PCR, Max Pain, Greeks

**Data flow:**
```
Feed â†’ market:* â†’ Worker â†’ enriched:* â†’ Socket Gateway â†’ Clients
```

**Benefits:**
- Decoupled: Feed doesn't know who consumes data
- Multi-consumer: Worker AND Logger both subscribe
- Asynchronous: Feed doesn't wait for processing
- Scalable: Add more subscribers without changing publisher"

**Follow-up Q: What are limitations of Redis Pub/Sub?**

"Great question. Redis Pub/Sub has significant limitations:

**1. No persistence:**
- If subscriber is offline, messages are lost
- No replay capability

**2. No acknowledgments:**
- Fire-and-forget, no confirmation of delivery
- If consumer crashes mid-processing, message lost

**3. No backpressure:**
- If subscriber is slow, messages buffer in Redis memory
- Can cause OOM if not careful

**4. At-most-once delivery:**
- Message delivered once or not at all
- No exactly-once guarantees

**Solutions:**
- For critical data: Use Celery (Redis as message broker) instead - has persistence, retries, acks
- For durability: Use Kafka - persistent log, replay capability
- For our use case: Acceptable because data is high-frequency and ephemeral (ticks), and we store in MongoDB for persistence

**Why we still use Pub/Sub:**
- Real-time data doesn't need persistence (new tick every second)
- Data already stored in MongoDB for history
- Simpler than Kafka for our scale
- Fast (< 1ms latency)"

---

### **Q5: What's the difference between Redis Pub/Sub and a message queue like RabbitMQ or Kafka?**

**Answer:**

**Redis Pub/Sub:**
- **Delivery**: Broadcasts to ALL subscribers (1-to-many)
- **Persistence**: No, messages lost if no subscriber
- **Acknowledgment**: No
- **Use case**: Real-time broadcasts (chat, live updates)

**Message Queue (RabbitMQ/Celery):**
- **Delivery**: ONE consumer processes message (1-to-1)
- **Persistence**: Yes, messages stored until consumed
- **Acknowledgment**: Yes, consumer acknowledges processing
- **Use case**: Task distribution (send email, process payment)

**Kafka:**
- **Delivery**: Consumer groups (each group gets copy, within group only one consumer)
- **Persistence**: Yes, durable log for days/weeks
- **Acknowledgment**: Yes, offset-based
- **Replay**: Can re-consume from any point
- **Use case**: Event sourcing, audit logs, data pipelines

**In our system:**
- **Redis Pub/Sub**: Market data broadcasting (Feed â†’ Workers)
- **Celery (Redis queue)**: Task processing (distribute work across workers)
- **MongoDB**: Persistence layer

**Example:**
```
Redis Pub/Sub:
Feed publishes 1 message â†’ Worker 1 receives it, Worker 2 receives it (both process)

Celery Queue:
Feed dispatches 1 task â†’ Worker 1 OR Worker 2 processes it (not both)
```

If I were to scale this for production, I'd replace Redis Pub/Sub with Kafka for durability and replay capability, but keep Celery for task distribution."

**Follow-up Q: When would you choose Kafka over Redis Pub/Sub?**

"I'd choose Kafka when:

1. **Data durability matters**: Financial transactions, audit logs, billing events
2. **Need to replay**: Debugging, reprocessing data, recovering from bugs
3. **High throughput**: 100K+ msg/sec (Kafka can handle millions)
4. **Multiple consumer groups**: Team A and Team B both need full stream
5. **Long-term storage**: Keep events for 30 days for analysis

I'd stick with Redis Pub/Sub when:
1. **Real-time, ephemeral**: Live scores, chat messages, stock ticks
2. **Simplicity**: Don't need Kafka's operational complexity
3. **Low latency**: <1ms latency (Kafka is ~10ms)
4. **Prototyping**: Faster to set up

For our trading platform, Redis Pub/Sub works because:
- Market ticks are ephemeral (new data every second)
- We persist in MongoDB anyway
- Low latency requirement (<100ms)
- Simpler ops (no Kafka cluster to manage)"

---

## ðŸ“š **SECTION 3: CELERY & ASYNC PROCESSING**

---

### **Q6: Explain how Celery works in your system. Walk me through task dispatch to completion.**

**Answer:**
"Celery is our distributed task queue for asynchronous processing. Here's the full flow:

**Architecture:**
```
Producer (Your App) â†’ Broker (Redis) â†’ Worker (Celery) â†’ Result Backend (Redis)
```

**Step-by-step for processing an option chain:**

**1. Task Definition:**
```python
@celery_app.task(base=EnrichmentTask, bind=True)
def process_option_chain(self, chain_data):
    # Calculate PCR
    # Calculate Max Pain
    # Store in MongoDB
    # Update cache
```

**2. Task Dispatch (Producer):**
```python
# Non-blocking call, returns immediately
task = process_option_chain.delay(chain_data)
print(f"Task ID: {task.id}")  # abc-123-def
# Code continues without waiting
```

**3. Message in Broker:**
Celery serializes task to JSON and pushes to Redis:
```json
{
  "id": "abc-123-def",
  "task": "app.process_option_chain",
  "args": [{"product": "NIFTY", "calls": [...], "puts": [...]}],
  "kwargs": {},
  "retries": 0
}
```

**4. Worker Picks Up Task:**
- Worker polls Redis broker
- Retrieves task message
- Deserializes JSON
- Executes `process_option_chain(chain_data)`

**5. Task Execution:**
```python
def process_option_chain(chain_data):
    # 1. Calculate analytics
    pcr = calculate_pcr(chain_data)
    max_pain = calculate_max_pain(chain_data)
    
    # 2. Store in MongoDB
    db.option_chains.insert_one(enriched_data)
    
    # 3. Update Redis cache
    redis_client.setex(f"latest:chain:{product}:{expiry}", 300, json.dumps(data))
    
    # 4. Publish enriched data
    redis_client.publish('enriched:option_chain', json.dumps(data))
    
    return {"status": "success", "pcr": pcr}
```

**6. Result Storage:**
- Worker stores result in Result Backend (Redis)
- Producer can retrieve: `result = task.get()`

**7. Task Acknowledgment:**
- With `acks_late=True`, worker ACKs only after successful completion
- If worker crashes mid-task, message stays in queue for retry

**Concurrency:**
- We run multiple workers (HPA: 2-10 replicas)
- Each worker processes one task at a time (`prefetch_multiplier=1`)
- Redis distributes tasks evenly across workers

This architecture allows us to process hundreds of option chains per second without blocking the API."

**Follow-up Q: What happens if the task fails?**

"We have a comprehensive retry and error handling strategy:

**1. Automatic Retry:**
```python
celery_app.conf.update(
    task_autoretry_for=(Exception,),
    task_retry_kwargs={'max_retries': 3, 'countdown': 5}
)
```

**Retry timeline:**
```
Attempt 1: Task fails (MongoDB timeout)
            â†“ Wait 5 seconds
Attempt 2: Task fails again
            â†“ Wait 10 seconds (exponential backoff)
Attempt 3: Task fails again
            â†“ Wait 20 seconds
Attempt 4: Task fails again
            â†“ Max retries exceeded
            â†“ Call on_failure() hook
```

**2. Dead Letter Queue:**
```python
def on_failure(self, exc, task_id, args, kwargs, einfo):
    dlq_message = {
        'task_id': task_id,
        'error': str(exc),
        'args': args,
        'timestamp': datetime.now().isoformat()
    }
    redis_client.lpush('dlq:enrichment', json.dumps(dlq_message))
    
    # Alert ops team
    send_alert(f"Task {task_id} failed after 3 retries")
```

**3. Monitoring DLQ:**
```bash
# Check DLQ size
redis-cli LLEN dlq:enrichment

# View failed tasks
redis-cli LRANGE dlq:enrichment 0 -1
```

**4. Manual Replay:**
```python
# Ops can manually reprocess
dlq_messages = redis_client.lrange('dlq:enrichment', 0, -1)
for msg in dlq_messages:
    task_data = json.loads(msg)
    process_option_chain.delay(*task_data['args'])
```

This ensures no data is silently lost, we're alerted to systemic issues, and we can recover from failures."

---

### **Q7: Explain exponential backoff. Why is it better than fixed retry?**

**Answer:**
"Exponential backoff means retry delays increase exponentially: 5s â†’ 10s â†’ 20s â†’ 40s.

**Our implementation:**
```python
task_retry_kwargs={'max_retries': 3, 'countdown': 5}
```

**Why exponential is better:**

**Scenario: MongoDB is temporarily overloaded**

**Fixed Retry (every 5s):**
```
Task 1 fails â†’ Retry after 5s â†’ Fails again â†’ Retry after 5s â†’ Fails
Task 2 fails â†’ Retry after 5s â†’ Fails again â†’ Retry after 5s â†’ Fails
Task 3 fails â†’ Retry after 5s â†’ Fails again â†’ Retry after 5s â†’ Fails
...
Result: 1000 tasks all retrying every 5s â†’ MongoDB gets HAMMERED
```

**Exponential Backoff:**
```
Task 1 fails â†’ Retry after 5s â†’ Fails â†’ Retry after 10s â†’ Succeeds (DB recovered)
Task 2 fails â†’ Retry after 5s â†’ Succeeds
Task 3 fails â†’ Retry after 5s â†’ Succeeds
...
Result: Gives MongoDB breathing room to recover
```

**Benefits:**
1. **Prevents thundering herd**: Not all tasks retry simultaneously
2. **Self-healing**: System has time to recover (connection pool refills, memory clears)
3. **Resource efficiency**: Fewer wasted retry attempts

**Real-world causes of transient failures:**
- Network hiccups (packet loss)
- Database connection pool exhaustion
- Temporary CPU spike
- Brief network partition
- Lock contention in MongoDB

Exponential backoff is industry standard for handling transient failures (used by AWS SDKs, Google APIs, etc.)."

**Follow-up Q: What's the maximum retry delay in your system?**

"Great question. Let me calculate:

```
Attempt 1: 5s delay
Attempt 2: 5s * 2^1 = 10s delay
Attempt 3: 5s * 2^2 = 20s delay
Total delay: 5 + 10 + 20 = 35 seconds before giving up
```

**Considerations for delay tuning:**

**Too short (3 retries in 15s):**
- System hasn't recovered yet
- Still overwhelms failing service
- More DLQ entries

**Too long (3 retries in 10 minutes):**
- Data freshness suffers
- Users see stale data
- Real issues take longer to detect

**Our 35s is good because:**
- Most transient issues resolve in <30s
- Trading data doesn't need to be >1min old
- If still failing after 35s, likely systemic (needs manual intervention)

**We could improve with:**
- Jitter: Random Â±20% to prevent synchronization
```python
import random
delay = base_delay * (2 ** attempt) * (1 + random.uniform(-0.2, 0.2))
```

- Max cap: Never wait more than 60s
```python
delay = min(base_delay * (2 ** attempt), 60)
```"

---

### **Q8: What is task idempotency and why is it critical?**

**Answer:**
"Idempotency means a task can be executed multiple times but produces the same result as executing it once.

**Why we need it:**

**Scenario without idempotency:**
```
1. Task processes NIFTY tick (tick_id=123)
2. Worker stores in MongoDB â†’ Success
3. Worker crashes before ACKing
4. Celery retries task (because no ACK)
5. Task processes NIFTY tick (tick_id=123) AGAIN
6. MongoDB has duplicate entry

Result: Double-counting in analytics, incorrect PCR
```

**Our implementation:**
```python
def process_underlying_tick(tick_data):
    tick_id = tick_data['tick_id']
    product = tick_data['product']
    
    # Idempotency check
    idempotency_key = f"processed:underlying:{product}:{tick_id}"
    if redis_client.exists(idempotency_key):
        logger.info("Already processed, skipping", tick_id=tick_id)
        return  # Safe to return, no side effects
    
    # Process the tick
    db.underlying_ticks.insert_one(tick_data)
    redis_client.setex(f"latest:underlying:{product}", 300, json.dumps(tick_data))
    
    # Mark as processed
    redis_client.setex(idempotency_key, 3600, '1')  # 1 hour TTL
```

**How it works:**
1. Before processing, check Redis: "Have I processed tick_id=123?"
2. If YES â†’ Skip (already done)
3. If NO â†’ Process and mark as processed

**TTL (1 hour):**
- After 1 hour, idempotency key expires
- Trade-off: Memory vs safety window
- 1 hour is sufficient because retries happen within seconds

**Why critical for distributed systems:**
- Network failures can cause duplicate messages
- Celery retries can reprocess same task
- Load balancers can duplicate requests
- Message brokers may deliver at-least-once

**At-least-once + Idempotency = Exactly-once semantics**"

**Follow-up Q: What if Redis goes down? Your idempotency checks fail.**

"You're right, that's a vulnerability. If Redis is down, idempotency checks fail-open, and we might process duplicates.

**Better solutions:**

**1. Database-level idempotency:**
```python
db.underlying_ticks.insert_one({
    'tick_id': tick_data['tick_id'],  # Unique index on tick_id
    'product': tick_data['product'],
    'price': tick_data['price']
})
# If duplicate, MongoDB raises DuplicateKeyError â†’ catch and skip
```

**2. Unique constraints:**
```python
db.underlying_ticks.create_index([('product', 1), ('tick_id', 1)], unique=True)
```

**3. Application-level state machine:**
```python
# Store processing state in DB
status = db.tick_processing.find_one({'tick_id': tick_id})
if status and status['state'] == 'completed':
    return

# Atomically update state
db.tick_processing.update_one(
    {'tick_id': tick_id},
    {'$set': {'state': 'processing'}},
    upsert=True
)

# Process...

db.tick_processing.update_one(
    {'tick_id': tick_id},
    {'$set': {'state': 'completed'}}
)
```

**Our trade-off:**
- Redis is faster than MongoDB (1ms vs 10ms)
- Redis is less critical (if down, we can tolerate duplicates briefly)
- For production, I'd use MongoDB unique constraints as backup

**Defense in depth:**
1. Redis idempotency (fast path)
2. MongoDB unique constraints (fallback)
3. Monitoring DLQ for duplicate errors"

---

### **Q9: Explain `acks_late=True`. Why is it important?**

**Answer:**
"`acks_late` controls when Celery acknowledges a task to the broker.

**Two modes:**

**Default (`acks_late=False` - Early ACK):**
```
1. Worker receives task from Redis
2. Worker sends ACK to Redis immediately â† ACK sent
3. Worker starts processing task
4. [Worker crashes during processing]
5. Task is LOST (already ACKed)
```

**Our setting (`acks_late=True` - Late ACK):**
```
1. Worker receives task from Redis
2. Worker starts processing task
3. Worker processes task (calculate PCR, store in DB)
4. Worker completes task successfully
5. Worker sends ACK to Redis â† ACK sent only after completion
```

**Why late ACK is critical:**

**Scenario: Worker crashes mid-processing**
```
With early ACK:
Task received â†’ ACK sent â†’ Processing... â†’ [CRASH]
Result: Task lost forever, data never processed

With late ACK:
Task received â†’ Processing... â†’ [CRASH] â†’ No ACK sent
Result: Redis still has task in queue â†’ Another worker picks it up
```

**Our implementation:**
```python
celery_app.conf.update(
    task_acks_late=True,                    # Don't ACK until done
    worker_prefetch_multiplier=1,           # One task at a time
    task_reject_on_worker_lost=True,        # Return to queue if crash
)
```

**Trade-off:**
- **Advantage**: No message loss on crash
- **Disadvantage**: If worker crashes, task is retried (need idempotency)

**Combined with idempotency:**
```
Task A processed â†’ Worker crashes before ACK â†’ Task A re-queued
â†’ New worker picks up â†’ Idempotency check: "Already processed" â†’ Skip
Result: No data loss, no duplicates
```

This is standard for at-least-once delivery in distributed systems."

**Follow-up Q: What if the task is slow? Could it timeout?**

"Yes, that's a risk. If a task runs longer than the broker's visibility timeout, the broker might think the worker is dead and re-queue the task.

**Celery's handling:**
- Celery sends heartbeats to broker while task is running
- Broker knows worker is alive and processing
- No timeout as long as heartbeats continue

**Problem cases:**
1. **Network partition**: Worker can't send heartbeats
2. **Infinite loop**: Task never completes
3. **Deadlock**: Task waiting on resource forever

**Solutions:**

**1. Task timeouts:**
```python
@celery_app.task(time_limit=300)  # Kill task after 5 minutes
def process_option_chain(data):
    ...
```

**2. Soft timeouts (graceful):**
```python
@celery_app.task(soft_time_limit=270, time_limit=300)
def process_option_chain(data):
    try:
        # Processing
    except SoftTimeLimitExceeded:
        # Cleanup, save partial results
        raise
```

**3. Monitoring:**
```python
# Alert if task takes >5 minutes
if task_duration > 300:
    send_alert(f"Task {task_id} slow: {task_duration}s")
```

**For our use case:**
- Most tasks complete in <100ms (simple analytics)
- Set timeout at 60s (60x expected duration)
- If task hits timeout, likely bug or data issue â†’ DLQ + investigate"

---

## ðŸ“š **SECTION 4: REDIS CACHING**

---

### **Q10: Explain your Redis caching strategy. Why multi-layer?**

**Answer:**
"We use a 3-level caching architecture with different TTLs and access patterns.

**Level 1: Hot Data (Real-time, 5min TTL)**
```python
latest:underlying:{product}      # Latest price for NIFTY
latest:option:{symbol}           # Latest quote for specific option
latest:chain:{product}:{expiry}  # Latest full option chain
latest:pcr:{product}:{expiry}    # Latest PCR values
```

**Level 2: Computed Data (Analytics, varies)**
```python
ohlc:{product}:{window}m         # OHLC for 1/5/15 min windows
volatility_surface:{product}     # IV surface (5min TTL)
```

**Level 3: Operational (1hr TTL)**
```python
processed:underlying:{product}:{tick_id}  # Idempotency tracking
dlq:enrichment                             # Dead letter queue
```

**Why multi-layer:**

**1. Different freshness requirements:**
- Latest prices: Update every second, 5min TTL okay
- OHLC windows: Recalculate every window (5min OHLC updates every 5min)
- Idempotency: Only needs to persist for retry window (1 hour)

**2. Memory optimization:**
- Hot data: Small keys, frequent access, short TTL â†’ Auto-cleanup
- Cold data: Longer TTL, less frequent cleanup

**3. Access patterns:**
- Level 1: Read-heavy (thousands of reads/sec)
- Level 2: Compute-heavy (expensive to regenerate)
- Level 3: Write-heavy (idempotency checks)

**Cache-aside implementation:**
```python
def get_underlying_price(product):
    # Try Level 1 cache
    cached = redis_client.get(f"latest:underlying:{product}")
    if cached:
        return json.loads(cached)  # Cache HIT
    
    # Cache MISS - query MongoDB
    data = db.underlying_ticks.find_one(
        {'product': product},
        sort=[('timestamp', -1)]
    )
    
    # Update cache with TTL
    redis_client.setex(
        f"latest:underlying:{product}",
        300,  # 5 minutes
        json.dumps(data)
    )
    
    return data
```

**Benefits:**
- Sub-50ms response time on cache hits
- Reduces MongoDB load (estimated 60% fewer queries)
- Auto-expiration prevents stale data"

**Follow-up Q: What's your cache hit ratio? How do you measure it?**

"We don't have production metrics (project not deployed), but here's how I'd measure:

**1. Instrument cache operations:**
```python
cache_hits = 0
cache_misses = 0

def get_with_metrics(key):
    global cache_hits, cache_misses
    
    cached = redis_client.get(key)
    if cached:
        cache_hits += 1
        return json.loads(cached)
    else:
        cache_misses += 1
        # Fetch from DB and cache
        ...
```

**2. Expose metrics endpoint:**
```python
@app.route('/metrics')
def metrics():
    hit_ratio = cache_hits / (cache_hits + cache_misses)
    return {
        'cache_hits': cache_hits,
        'cache_misses': cache_misses,
        'hit_ratio': hit_ratio
    }
```

**3. Expected hit ratios:**
- **Level 1 (latest prices)**: 90-95% (frequently requested)
- **Level 2 (OHLC)**: 70-80% (computed data, less frequent)
- **Level 3 (idempotency)**: 5-10% (mostly writes, few duplicate checks)

**4. Optimization if hit ratio low:**
- Increase TTL (trade-off with freshness)
- Pre-warm cache (populate before requests)
- Predictive caching (fetch what user likely needs next)

**5. Prometheus metrics:**
```python
from prometheus_client import Counter, Histogram

cache_hit_counter = Counter('cache_hits_total', 'Total cache hits')
cache_miss_counter = Counter('cache_misses_total', 'Total cache misses')
cache_latency = Histogram('cache_latency_seconds', 'Cache operation latency')

@cache_latency.time()
def get_with_metrics(key):
    cached = redis_client.get(key)
    if cached:
        cache_hit_counter.inc()
    else:
        cache_miss_counter.inc()
    ...
```

This would let us visualize hit ratio over time in Grafana."

---

### **Q11: How do you handle cache invalidation?**

**Answer:**
"Cache invalidation is 'one of the two hard problems in computer science.' We use TTL-based expiration with event-driven invalidation.

**Strategy 1: TTL-based (Lazy invalidation)**
```python
redis_client.setex(key, 300, value)  # Auto-expires after 5 min
```

**Benefits:**
- Simple, automatic cleanup
- No manual invalidation logic
- Memory efficient

**Drawback:**
- Stale data for up to 5 minutes

**Strategy 2: Event-driven (Active invalidation)**
```python
def process_option_chain(chain_data):
    product = chain_data['product']
    expiry = chain_data['expiry']
    
    # Process and calculate
    enriched = calculate_analytics(chain_data)
    
    # Update MongoDB
    db.option_chains.insert_one(enriched)
    
    # Update cache with fresh data (invalidate by overwriting)
    redis_client.setex(
        f"latest:chain:{product}:{expiry}",
        300,
        json.dumps(enriched)
    )
```

**This is 'cache-aside with write-through invalidation':**
- Write to database
- Update cache immediately (invalidate old)
- Ensures cache is always fresh

**Strategy 3: Pattern-based invalidation**
```python
def invalidate_product_caches(product):
    # Delete all keys matching pattern
    pattern = f"*:{product}:*"
    keys = redis_client.keys(pattern)
    if keys:
        redis_client.delete(*keys)
```

**Use cases:**
- User updates product settings â†’ Invalidate all related caches
- System detects bad data â†’ Purge all caches

**Strategy 4: Cache versioning**
```python
CACHE_VERSION = 'v2'
redis_client.set(f"latest:underlying:{product}:{CACHE_VERSION}", data)

# On schema change, bump version â†’ Old caches auto-expire
CACHE_VERSION = 'v3'  # v2 caches now orphaned, will expire via TTL
```

**Our hybrid approach:**
- **Normal case**: TTL-based (5min) â†’ Simple
- **Data update**: Event-driven (immediate overwrite) â†’ Fast
- **Emergency**: Pattern-based deletion â†’ Manual intervention

**Trade-offs:**
- **Short TTL (1min)**: Fresh data, more DB load
- **Long TTL (1hr)**: Less DB load, stale data
- **Our 5min**: Balance for trading data (not critical to be <5min fresh)"

**Follow-up Q: What about cache stampede (thundering herd)?**

"Great question! Cache stampede happens when:
1. Popular cache key expires
2. 1000 requests arrive simultaneously
3. All 1000 find cache miss
4. All 1000 query database simultaneously
5. Database overwhelmed

**Our vulnerability:**
```python
cached = redis_client.get(f"latest:chain:{product}:{expiry}")
if not cached:
    # 1000 requests reach here simultaneously
    data = db.option_chains.find_one(...)  # 1000 DB queries!
    redis_client.setex(key, 300, json.dumps(data))
```

**Solution 1: Locking (pessimistic)**
```python
import redis_lock

lock = redis_lock.Lock(redis_client, f"lock:chain:{product}:{expiry}")
if lock.acquire(blocking=False):
    try:
        # Only ONE request queries DB
        data = db.option_chains.find_one(...)
        redis_client.setex(key, 300, json.dumps(data))
    finally:
        lock.release()
else:
    # Other requests wait for cache to be populated
    time.sleep(0.1)
    cached = redis_client.get(key)
```

**Solution 2: Probabilistic early expiration**
```python
import random

ttl = redis_client.ttl(key)
expiry_threshold = 300 * 0.1  # 10% of TTL (30 seconds)

if ttl < expiry_threshold and random.random() < 0.1:
    # 10% chance to refresh if TTL < 30s
    data = db.option_chains.find_one(...)
    redis_client.setex(key, 300, json.dumps(data))
```

**Solution 3: Background refresh**
```python
@celery_app.task
def refresh_cache_task(product, expiry):
    data = db.option_chains.find_one({'product': product, 'expiry': expiry})
    redis_client.setex(f"latest:chain:{product}:{expiry}", 300, json.dumps(data))

# Scheduled task refreshes popular caches before expiry
celery_app.conf.beat_schedule = {
    'refresh-nifty-cache': {
        'task': 'refresh_cache_task',
        'schedule': 240.0,  # Every 4 min (before 5min expiry)
        'args': ('NIFTY', '2025-01-25')
    }
}
```

**For production, I'd use Solution 1 (locking) for critical paths and Solution 3 (background refresh) for frequently accessed data."

---

## ðŸ“š **SECTION 5: WEBSOCKET & REAL-TIME DATA**

---

### **Q12: How does your WebSocket architecture work? How do you scale it?**

**Answer:**
"We use Flask-SocketIO for real-time data streaming with horizontal scaling via Redis message queue.

**Basic architecture:**
```python
# Initialize Socket.IO with Redis backend
socketio = SocketIO(
    app,
    cors_allowed_origins="*",
    message_queue=redis_url,  # Critical for scaling
    async_mode='threading'
)
```

**Client connection flow:**
```python
# 1. Client connects
@socketio.on('connect')
def handle_connect():
    client_id = request.sid
    join_room('general')  # Auto-join global room
    emit('connected', {'client_id': client_id})

# 2. Client subscribes to NIFTY
@socketio.on('subscribe')
def handle_subscribe(data):
    symbol = data['symbol']  # 'NIFTY'
    room = f"product:{symbol}"
    join_room(room)  # Client now in 'product:NIFTY' room
```

**Data broadcasting flow:**
```python
# Background thread listens to Redis Pub/Sub
def redis_listener():
    pubsub = redis_client.pubsub()
    pubsub.subscribe('enriched:underlying')
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            data = json.loads(message['data'])
            product = data['product']
            
            # Broadcast to product-specific room
            socketio.emit('underlying_update', data, room=f"product:{product}")
```

**Horizontal scaling (Multiple instances):**

**Without Redis message queue:**
```
Instance 1 (Clients A, B) â†’ Receives data â†’ Broadcasts to A, B
Instance 2 (Clients C, D) â†’ Receives data â†’ Broadcasts to C, D

Problem: If data arrives at Instance 1, only A and B get it. C and D miss it.
```

**With Redis message queue:**
```
Instance 1 â†’ socketio.emit() â†’ Redis
                                 â†“
                        Broadcasts to all instances
                                 â†“
             Instance 1 â† Redis â†’ Instance 2
             (A, B get it)        (C, D get it)
```

**Implementation:**
```python
# Instance 1 emits
socketio.emit('underlying_update', data, room='product:NIFTY')

# Redis distributes to all Socket.IO instances
# All instances broadcast to their connected clients
```

**Room-based targeting:**
```
Client A subscribes to 'product:NIFTY'
Client B subscribes to 'product:BANKNIFTY'
Client C subscribes to 'product:NIFTY'

Broadcast to 'product:NIFTY' â†’ Only A and C receive
```

**Scaling in Kubernetes:**
```yaml
# Multiple Socket Gateway pods
replicas: 3

# Clients load-balanced across pods
# Redis message queue ensures consistency
```

This architecture allows us to scale to thousands of concurrent connections by adding more Socket Gateway instances."

**Follow-up Q: How do you handle reconnections?**

"Socket.IO has built-in reconnection logic, but we add application-level handling:

**Client-side:**
```javascript
const socket = io('http://localhost:8002', {
    reconnection: true,
    reconnectionDelay: 1000,      // Start at 1s
    reconnectionDelayMax: 5000,   // Max 5s
    reconnectionAttempts: 5
});

socket.on('connect', () => {
    // Re-subscribe after reconnection
    socket.emit('subscribe', {type: 'product', symbol: 'NIFTY'});
});

socket.on('disconnect', (reason) => {
    if (reason === 'io server disconnect') {
        // Server forced disconnect, manual reconnect
        socket.connect();
    }
    // Otherwise, auto-reconnect
});
```

**Server-side:**
```python
@socketio.on('connect')
def handle_connect():
    client_id = request.sid
    
    # Check if reconnecting client
    previous_session = redis_client.get(f"session:{client_id}")
    if previous_session:
        # Restore subscriptions
        subscriptions = json.loads(previous_session)
        for room in subscriptions['rooms']:
            join_room(room)
        
        emit('reconnected', {'restored_rooms': subscriptions['rooms']})
    else:
        # New connection
        join_room('general')
        emit('connected', {'client_id': client_id})

@socketio.on('disconnect')
def handle_disconnect():
    client_id = request.sid
    
    # Save session for potential reconnection
    session_data = {
        'rooms': list(rooms()),
        'timestamp': datetime.now().isoformat()
    }
    redis_client.setex(f"session:{client_id}", 300, json.dumps(session_data))
```

**Handling missed messages:**
```python
@socketio.on('subscribe')
def handle_subscribe(data):
    symbol = data['symbol']
    room = f"product:{symbol}"
    join_room(room)
    
    # Send latest cached data immediately
    cached = redis_client.get(f"latest:underlying:{symbol}")
    if cached:
        emit('underlying_update', json.loads(cached))
```

**Exponential backoff:**
1s â†’ 2s â†’ 4s â†’ 5s (max)

This ensures clients reconnect gracefully without overwhelming the server during outages."

---

### **Q13: What's the difference between WebSocket and HTTP polling? Why use WebSocket?**

**Answer:**

**HTTP Polling (Old way):**
```javascript
// Client polls every 1 second
setInterval(() => {
    fetch('/api/data/underlying/NIFTY')
        .then(res => res.json())
        .then(data => updateUI(data));
}, 1000);
```

**Problems:**
1. **Latency**: Average 500ms delay (poll interval / 2)
2. **Bandwidth waste**: 1 request/sec even if no new data
3. **Server load**: 1000 clients = 1000 req/sec
4. **Battery drain**: Mobile devices constantly requesting

**WebSocket (Our approach):**
```javascript
const socket = io('http://localhost:8002');
socket.on('underlying_update', (data) => {
    updateUI(data);  // Instant update when data available
});
```

**Benefits:**
1. **Low latency**: <100ms (push immediately when data arrives)
2. **Efficient**: Server pushes only when there's new data
3. **Bi-directional**: Server can push, client can send
4. **Persistent connection**: One connection, many messages

**Comparison:**

**HTTP Polling:**
- Request overhead: ~500 bytes (headers) per request
- 1000 clients Ã— 1 req/sec Ã— 500 bytes = 500 KB/sec bandwidth
- Plus response data (another 1 KB) = 1.5 MB/sec total

**WebSocket:**
- Initial handshake: ~500 bytes (once)
- Message overhead: ~10 bytes per message
- 1000 clients Ã— 1 msg/sec Ã— 10 bytes = 10 KB/sec
- Plus message data (1 KB) = 1 MB/sec total

**~30% bandwidth savings**

**Why WebSocket for trading:**
- Real-time prices (latency matters)
- Frequent updates (1+ per second)
- Scalability (thousands of connections)
- User experience (live updates, no lag)

**When HTTP polling is okay:**
- Infrequent updates (weather app, hourly check)
- Simple deployment (no WebSocket infrastructure)
- Caching helps (same data for all users)

For our real-time trading platform, WebSocket is the only viable option."

**Follow-up Q: How do you monitor WebSocket connections?**

"We track connection metrics and expose them via `/metrics` endpoint:

```python
# Track connected clients
connected_clients = {}

@socketio.on('connect')
def handle_connect():
    client_id = request.sid
    connected_clients[client_id] = {
        'connected_at': time.time(),
        'rooms': ['general']
    }

@app.route('/metrics')
def metrics():
    # Total connections
    total_clients = len(connected_clients)
    
    # Connections per room
    room_counts = {}
    for client_id, client_info in connected_clients.items():
        for room in client_info['rooms']:
            room_counts[room] = room_counts.get(room, 0) + 1
    
    # Connection duration
    durations = [time.time() - c['connected_at'] for c in connected_clients.values()]
    avg_duration = sum(durations) / len(durations) if durations else 0
    
    return {
        'total_clients': total_clients,
        'rooms': room_counts,
        'avg_connection_duration_seconds': avg_duration
    }
```

**Prometheus metrics:**
```python
from prometheus_client import Gauge, Counter

websocket_connections = Gauge('websocket_connections_total', 'Total WebSocket connections')
websocket_messages_sent = Counter('websocket_messages_sent_total', 'Total messages sent')
websocket_messages_received = Counter('websocket_messages_received_total', 'Total messages received')

@socketio.on('connect')
def handle_connect():
    websocket_connections.inc()

@socketio.on('disconnect')
def handle_disconnect():
    websocket_connections.dec()

def redis_listener():
    for message in pubsub.listen():
        socketio.emit('update', data)
        websocket_messages_sent.inc()
```

**Grafana dashboard:**
- Total connections over time
- Connections per room (which products are popular)
- Messages/sec throughput
- Connection churn rate (connects - disconnects)

**Alerting:**
```python
if total_clients > 5000:
    send_alert("WebSocket connections > 5000, consider scaling")

if avg_connection_duration < 60:
    send_alert("High churn rate, investigate connection stability")
```"

---

## ðŸ“š **SECTION 6: KUBERNETES & DEPLOYMENT**

---

### **Q14: Explain your Kubernetes deployment. How does HPA work?**

**Answer:**
"We deploy the platform on Kubernetes with Horizontal Pod Autoscaler for dynamic scaling.

**Deployment manifest:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker-enricher
spec:
  replicas: 5  # Default replicas
  selector:
    matchLabels:
      app: worker-enricher
  template:
    spec:
      containers:
      - name: worker-enricher
        image: option-aro/worker-enricher:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

**HPA Configuration:**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-enricher-hpa
spec:
  scaleTargetRef:
    kind: Deployment
    name: worker-enricher
  minReplicas: 2   # Never fewer than 2
  maxReplicas: 10  # Never more than 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 70  # Target 70% CPU
  - type: Resource
    resource:
      name: memory
      target:
        averageUtilization: 80  # Target 80% memory
```

**How HPA works:**

**1. Monitoring:**
- HPA queries metrics-server every 15 seconds
- Gets current CPU/memory usage per pod

**2. Decision logic:**
```
desired_replicas = current_replicas Ã— (current_metric / target_metric)

Example:
current_replicas = 5
current_cpu = 85%
target_cpu = 70%

desired = 5 Ã— (85 / 70) = 6.07 â†’ Scale to 6 replicas
```

**3. Scaling actions:**
```
Current: 2 replicas, CPU at 80%
â†“ HPA scales up
Now: 3 replicas, CPU at 60% (below 70% target)
â†“ Stable for 5 minutes
â†“ Load decreases, CPU at 40%
â†“ HPA scales down
Now: 2 replicas, CPU at 60%
```

**4. Cooldown:**
- Scale-up: Immediate (handle load spikes)
- Scale-down: 5 min cooldown (avoid thrashing)

**Real-world scenario:**

**Market opens (9:15 AM):**
```
8:00 AM: 2 replicas, low volume
9:15 AM: Market opens, volume spikes
9:16 AM: CPU hits 90%, HPA scales to 5 replicas
9:17 AM: CPU drops to 65%, stable
3:30 PM: Market closes, volume drops
3:35 PM: CPU at 30%, HPA scales down to 3 replicas
3:40 PM: Still low, scales down to 2 replicas
```

**Benefits:**
- **Cost efficiency**: Pay only for what you need
- **Automatic**: No manual scaling
- **Responsive**: Handles unexpected load
- **Resilience**: If pod crashes, HPA maintains desired count

**Other services:**
- API Gateway: 2-5 replicas (lower variability)
- Socket Gateway: 3-8 replicas (connection-dependent)
- Workers: 2-10 replicas (highest variability)"

**Follow-up Q: What if scaling isn't fast enough for sudden spikes?**

"Good question. HPA reactive scaling has a lag:
1. Metrics collected every 15s
2. Decision made
3. Pod creation takes 30-60s (image pull, container start)
4. Pod registers with service

Total: ~90 seconds to scale up

**Solutions:**

**1. Vertical Pod Autoscaler (VPA):**
```yaml
# Increase resource limits, not replicas
requests:
  cpu: "250m" â†’ "500m"  # Give more CPU to existing pods
```

**2. Pre-warming:**
```yaml
minReplicas: 5  # Start with higher baseline
# Accept slightly higher cost for faster response
```

**3. Predictive scaling (custom metrics):**
```python
# Scale based on queue depth, not CPU
from kubernetes import client, config

queue_depth = redis_client.llen('celery')
if queue_depth > 100:
    # Scale up proactively
    scale_deployment('worker-enricher', replicas=8)
```

**4. Cluster Autoscaler:**
```
HPA scales pods: 5 â†’ 10 replicas
â†“ Not enough nodes to schedule 10 pods
â†“ Cluster Autoscaler adds nodes
â†“ Pods scheduled on new nodes
```

**5. Lower CPU threshold:**
```yaml
target:
  averageUtilization: 50  # Scale earlier (more headroom)
```

**6. Scheduled scaling (cron):**
```python
# Scale up before market opens
# 8:45 AM: Scale to 8 replicas (pre-warm for 9:15 AM open)
# 4:00 PM: Scale down to 2 replicas (post market close)
```

**For production trading system, I'd use:**
- Pre-warmed baseline (5 replicas minimum during market hours)
- Queue-depth based custom metrics (more responsive than CPU)
- Scheduled scaling (predictable daily patterns)
- Lower CPU threshold (60% instead of 70%)"

---

## ðŸ“š **SECTION 7: OBSERVABILITY & MONITORING**

---

### **Q15: How do you debug issues in a distributed system with 8 services?**

**Answer:**
"Debugging distributed systems is challenging because a request spans multiple services. We use structured logging, trace IDs, and centralized metrics.

**Problem: User reports 'NIFTY option chain not loading'**

**Step 1: Check API Gateway logs**
```bash
docker logs api-gateway | grep NIFTY
```

**Without structured logging (hard to parse):**
```
ERROR - Chain request failed for NIFTY 2025-01-25
```

**With structured logging (easy to query):**
```json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "error",
  "service": "api-gateway",
  "endpoint": "/api/data/chain/NIFTY",
  "product": "NIFTY",
  "expiry": "2025-01-25",
  "error": "Storage service unavailable",
  "status_code": 503,
  "trace_id": "abc-123-def"
}
```

**Now I know:**
- Which service failed: Storage
- Error type: Service unavailable
- Trace ID to follow request through system

**Step 2: Check Storage Service logs (using trace_id)**
```bash
docker logs storage | grep abc-123-def
```

```json
{
  "timestamp": "2025-01-15T10:30:00.123Z",
  "level": "error",
  "service": "storage",
  "operation": "get_option_chain",
  "product": "NIFTY",
  "error": "MongoDB connection timeout",
  "trace_id": "abc-123-def"
}
```

**Root cause: MongoDB timeout**

**Step 3: Check MongoDB**
```bash
docker logs mongodb

# See high connection count
connections: 95/100 (near limit)
```

**Step 4: Check Worker logs (they might be overwhelming MongoDB)**
```bash
docker logs worker-enricher

# See thousands of option chains being processed
processed_option_chain: 1523 chains in last minute
```

**Root cause identified:**
- Workers processing too many chains
- MongoDB connection pool exhausted
- Storage service can't get connections

**Solution:**
1. Increase MongoDB connection pool: 100 â†’ 200
2. Rate-limit worker processing
3. Add MongoDB read replica for load distribution

**Our structured logging implementation:**
```python
import structlog
import uuid

# Add trace ID to context
def add_trace_id(logger, method_name, event_dict):
    if 'trace_id' not in event_dict:
        event_dict['trace_id'] = str(uuid.uuid4())
    return event_dict

structlog.configure(
    processors=[
        add_trace_id,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()

# Usage
logger.error(
    "chain_request_failed",
    product="NIFTY",
    expiry="2025-01-25",
    error=str(e),
    trace_id=request_id
)
```

**Propagating trace ID across services:**
```python
# API Gateway forwards trace ID
response = requests.get(
    f"{STORAGE_SERVICE_URL}/option/chain/{product}",
    headers={'X-Trace-ID': trace_id}
)

# Storage Service extracts trace ID
trace_id = request.headers.get('X-Trace-ID', str(uuid.uuid4()))
logger.info("processing_request", trace_id=trace_id)
```

**Centralized logging (production):**
```
All services â†’ Fluentd/Filebeat â†’ Elasticsearch â†’ Kibana
                                     (searchable)  (visualize)

Query: trace_id:"abc-123-def"
Result: All logs for that request across all services
```

This approach reduces debugging time from hours to minutes."

**Follow-up Q: What metrics do you monitor?**

"We monitor four categories:

**1. Service Health (Uptime, errors)**
```python
@app.route('/health')
def health():
    return {'status': 'healthy', 'service': 'api-gateway'}

# Prometheus scrapes /health every 15s
# Alert if service down for >1 minute
```

**2. Performance (Latency, throughput)**
```python
from prometheus_client import Histogram, Counter

request_latency = Histogram('request_duration_seconds', 'Request latency')
request_count = Counter('requests_total', 'Total requests', ['endpoint', 'status'])

@app.route('/api/data/chain/<product>')
@request_latency.time()
def get_chain(product):
    result = process_request()
    request_count.labels(endpoint='/api/data/chain', status='200').inc()
    return result

# Alert if p95 latency > 500ms
# Alert if throughput < 100 req/min (traffic drop)
```

**3. Business Metrics (Application-specific)**
```python
# Trading-specific metrics
chains_processed = Counter('option_chains_processed_total', 'Chains processed')
pcr_gauge = Gauge('latest_pcr', 'Latest PCR value', ['product'])

def process_option_chain(data):
    chains_processed.inc()
    pcr_gauge.labels(product=data['product']).set(data['pcr'])

# Alert if chains_processed = 0 for 5 min (feed generator down)
# Alert if PCR > 2.0 (unusual market sentiment)
```

**4. Infrastructure (CPU, memory, connections)**
```python
# Kubernetes metrics (via metrics-server)
- Pod CPU usage
- Pod memory usage
- Node capacity

# Custom metrics
websocket_connections = Gauge('websocket_connections', 'Active WebSocket connections')
celery_queue_depth = Gauge('celery_queue_depth', 'Tasks in Celery queue')

# Alert if memory > 80%
# Alert if CPU > 90% for >5 min
# Alert if queue depth > 1000 (workers can't keep up)
```

**Grafana Dashboard:**
```
Row 1: Service Health (API Gateway, Storage, Workers)
Row 2: Request Latency (p50, p95, p99)
Row 3: Throughput (Requests/sec, Chains/sec)
Row 4: Business Metrics (PCR, Active products, Subscriptions)
Row 5: Infrastructure (CPU, Memory, Network)
```

**Alerting rules:**
```yaml
# Prometheus alert rules
groups:
  - name: option-aro-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        annotations:
          summary: "High error rate: {{ $value }}%"
      
      - alert: HighLatency
        expr: histogram_quantile(0.95, request_duration_seconds) > 0.5
        for: 5m
        annotations:
          summary: "P95 latency > 500ms"
```"

---

This guide covers the core technical concepts and interview questions. Would you like me to continue with more sections on specific topics like:
- Option Greeks calculations
- MongoDB indexing strategies  
- Docker best practices
- System design questions
- Behavioral questions about your project?
Action: finish
Observation: 
 Summary: # ðŸ“š Complete Technical Deep Dive & Interview Prep Guide

---

## ðŸŽ¯ PART 1: TECHNICAL CONCEPTS EXPLAINED IN DEPTH

---

### **1. DISTRIBUTED MICROSERVICES ARCHITECTURE**

**What it is:**
Breaking a monolithic application into small, independent services where each service handles a specific business capability and runs in its own process.

**In your project:**
- **8 services**: API Gateway, Socket Gateway, Worker Enricher, Feed Generator, Storage, Auth, Analytics, Logging
- Each service has its own codebase, can be deployed independently, and communicates via HTTP REST APIs or Redis Pub/Sub

**Why it matters:**
- **Scalability**: Can scale individual services based on load (e.g., scale workers without scaling API gateway)
- **Fault Isolation**: If one service crashes, others continue working
- **Technology Flexibility**: Each service can use different tech stack
- **Team Autonomy**: Different teams can own different services

**Key characteristics in your code:**
```python
# Each service runs independently on different ports
API Gateway: 8000
Auth: 8001
Socket Gateway: 8002
Storage: 8003
Analytics: 8004
Logging: 8005
```

---

### **2. EVENT-DRIVEN ARCHITECTURE**

**What it is:**
Services communicate by publishing and subscribing to events rather than direct API calls. When something happens, an event is published, and interested services react to it.

**In your project:**
```python
# Feed Generator publishes events
redis_client.publish('market:underlying', json.dumps(tick_data))
redis_client.publish('market:option_chain', json.dumps(chain_data))

# Worker Enricher subscribes to events
pubsub.subscribe('market:underlying', 'market:option_quote', 'market:option_chain')
```

**Benefits:**
- **Loose Coupling**: Services don't need to know about each other
- **Asynchronous**: Publisher doesn't wait for subscribers
- **Multiple Consumers**: Many services can react to same event
- **Resilience**: If a subscriber is down, events aren't lost (Redis handles buffering)

**5 Channels in your project:**
1. `market:underlying` - Raw underlying price ticks
2. `market:option_quote` - Individual option quotes
3. `market:option_chain` - Complete option chains
4. `enriched:underlying` - Processed underlying data
5. `enriched:option_chain` - Enriched chain with analytics

---

### **3. REDIS PUB/SUB**

**What it is:**
Redis Publish/Subscribe is a messaging pattern where publishers send messages to channels without knowing who will receive them, and subscribers listen to channels of interest.

**How it works:**
```
Publisher â†’ Redis Channel â†’ Multiple Subscribers
(Fire and forget)      (Message broker)     (All receive message)
```

**In your code:**
```python
# Publisher (Feed Generator)
redis_client.publish('market:underlying', json.dumps({
    'product': 'NIFTY',
    'price': 21543.25,
    'timestamp': '2025-01-15T10:30:00Z'
}))

# Subscriber (Worker Enricher)
pubsub = redis_client.pubsub()
pubsub.subscribe('market:underlying')
for message in pubsub.listen():
    if message['type'] == 'message':
        data = json.loads(message['data'])
        process_underlying_tick.delay(data)  # Dispatch to Celery
```

**Key Differences from Queue:**
- **Pub/Sub**: Message delivered to ALL subscribers (1-to-many)
- **Queue**: Message delivered to ONE consumer (1-to-1)

---

### **4. CELERY - DISTRIBUTED TASK QUEUE**

**What it is:**
Celery is a distributed task queue system that allows you to run functions asynchronously in background workers.

**Architecture:**
```
Producer â†’ Broker (Redis) â†’ Worker â†’ Result Backend (Redis)
(Your app)  (Queue)        (Celery)   (Stores results)
```

**In your code:**
```python
# Define task
@celery_app.task
def process_option_chain(chain_data):
    # Heavy computation here
    calculate_pcr()
    calculate_max_pain()
    store_in_mongodb()

# Dispatch task (non-blocking)
process_option_chain.delay(chain_data)  # Returns immediately
```

**Why use Celery:**
- Offload heavy computations from API requests
- Retry failed tasks automatically
- Scale workers independently
- Handle long-running tasks without blocking

**Your Celery configuration:**
```python
celery_app.conf.update(
    task_serializer='json',
    task_acks_late=True,              # Critical for reliability
    worker_prefetch_multiplier=1,     # One task at a time per worker
    task_reject_on_worker_lost=True,  # Re-queue if worker crashes
)
```

---

### **5. EXPONENTIAL BACKOFF RETRY**

**What it is:**
When a task fails, retry it with increasing delays: 5s â†’ 10s â†’ 20s â†’ 40s

**Why exponential:**
- Prevents overwhelming failing service with retries
- Gives time for transient issues to resolve (network hiccup, DB lock)
- Reduces thundering herd problem

**In your code:**
```python
celery_app.conf.update(
    task_autoretry_for=(Exception,),           # Retry on any exception
    task_retry_kwargs={'max_retries': 3, 'countdown': 5},  # Max 3, start at 5s
    task_default_retry_delay=5,
)
```

**Retry timeline:**
```
Attempt 1: Fails â†’ Wait 5s
Attempt 2: Fails â†’ Wait 10s (2^1 * 5s)
Attempt 3: Fails â†’ Wait 20s (2^2 * 5s)
Attempt 4: Fails â†’ Send to DLQ
```

**Better than fixed retry:**
- Fixed: Retry every 5s â†’ Can overwhelm system
- Exponential: Increasing delays â†’ Gives system breathing room

---

### **6. DEAD LETTER QUEUE (DLQ)**

**What it is:**
A special queue where messages that failed after all retry attempts are stored for later inspection/manual handling.

**In your code:**
```python
def on_failure(self, exc, task_id, args, kwargs, einfo):
    # After max retries exhausted, send to DLQ
    dlq_message = {
        'task_id': task_id,
        'task_name': self.name,
        'error': str(exc),
        'args': args,
        'timestamp': datetime.now().isoformat()
    }
    redis_client.lpush('dlq:enrichment', json.dumps(dlq_message))
```

**Why DLQ is critical:**
- **No data loss**: Failed messages aren't discarded
- **Debugging**: Inspect what went wrong
- **Replay capability**: Can manually re-process later
- **Alerting**: Monitor DLQ size to detect issues

**Viewing DLQ:**
```bash
redis-cli LRANGE dlq:enrichment 0 -1
```

---

### **7. TASK IDEMPOTENCY**

**What it is:**
Ensuring a task can be executed multiple times without changing the result beyond the first execution.

**Why it matters:**
- Network issues can cause duplicate messages
- Retries can process same task twice
- Worker crashes can lead to re-processing

**In your code:**
```python
def process_underlying_tick(tick_data):
    tick_id = tick_data['tick_id']
    product = tick_data['product']
    
    # Check if already processed
    idempotency_key = f"processed:underlying:{product}:{tick_id}"
    if redis_client.exists(idempotency_key):
        return  # Already processed, skip
    
    # Process the tick
    store_in_mongodb(tick_data)
    update_cache(tick_data)
    
    # Mark as processed (TTL 1 hour)
    redis_client.setex(idempotency_key, 3600, '1')
```

**Without idempotency:**
- Same tick processed twice â†’ Duplicate DB entries
- Analytics calculated multiple times â†’ Incorrect metrics

---

### **8. LATE ACKNOWLEDGMENT PATTERN (acks_late=True)**

**What it is:**
Task is acknowledged to broker only AFTER successful completion, not when worker receives it.

**Two modes:**
```
Early Ack (default):
Worker receives task â†’ Send ACK â†’ Process task â†’ Done
Problem: If worker crashes during processing, task is lost

Late Ack (acks_late=True):
Worker receives task â†’ Process task â†’ Send ACK â†’ Done
Benefit: If worker crashes, task is NOT acked, so broker re-queues it
```

**In your code:**
```python
celery_app.conf.update(
    task_acks_late=True,  # Don't ack until task completes
)
```

**Why it matters:**
- Prevents message loss on worker crash
- Ensures at-least-once delivery
- Combined with idempotency = exactly-once processing

---

### **9. REDIS CACHING - CACHE-ASIDE PATTERN**

**What it is:**
Application code is responsible for reading from cache and updating it.

**Flow:**
```
1. Request arrives
2. Check cache (Redis)
   â”œâ”€ Cache HIT â†’ Return cached data
   â””â”€ Cache MISS â†’ Query database â†’ Store in cache â†’ Return data
```

**In your code:**
```python
def get_underlying(product):
    # Try cache first
    cache_key = f"latest:underlying:{product}"
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)  # Cache HIT
    
    # Cache MISS - query database
    data = db.underlying_ticks.find_one({'product': product})
    
    # Update cache with TTL
    redis_client.setex(cache_key, 300, json.dumps(data))  # 5 min TTL
    
    return data
```

**Alternative patterns:**
- **Write-through**: Write to cache + DB simultaneously
- **Write-behind**: Write to cache, async write to DB later

**Why cache-aside:**
- Application controls what/when to cache
- Read-heavy workloads (your use case)
- DB is source of truth

---

### **10. TTL (TIME TO LIVE)**

**What it is:**
Automatic expiration of cached data after specified time.

**In your code:**
```python
# Cache with 5-minute TTL
redis_client.setex(f"latest:underlying:{product}", 300, json.dumps(data))
                                                    ^^^
                                                  300 seconds = 5 min
```

**Why TTL:**
- **Memory management**: Old data auto-deleted
- **Freshness**: Forces re-fetch of stale data
- **No manual invalidation**: Redis handles cleanup

**TTL strategy in your project:**
```
Hot data (latest prices): 5 min TTL
OHLC windows: Window duration TTL (5min window = 5min TTL)
Idempotency keys: 1 hour TTL
```

---

### **11. MULTI-LEVEL CACHING (3 CACHE LEVELS)**

**What it is:**
Multiple cache layers with different data and TTLs.

**Your 3 levels:**

**Level 1: Latest Data (Real-time, 5min TTL)**
```python
latest:underlying:{product}      # Latest price
latest:option:{symbol}           # Latest quote
latest:chain:{product}:{expiry}  # Latest chain
```

**Level 2: Computed Data (Analytics, varies)**
```python
ohlc:{product}:{window}m         # OHLC windows
volatility_surface:{product}     # IV surface
```

**Level 3: Operational Data (1hr TTL)**
```python
processed:underlying:{product}:{tick_id}  # Idempotency tracking
```

**Why multi-level:**
- Different data has different freshness requirements
- Separate hot path from cold path
- Optimize memory usage

---

### **12. REDIS SORTED SETS**

**What it is:**
Redis data structure where each member has a score, automatically sorted by score.

**In your code:**
```python
# Store IV surface data (score = strike price)
redis_client.zadd(
    f"iv_surface:{product}",
    {
        json.dumps({'strike': 21500, 'iv': 0.25, 'expiry': '2025-01-25'}): 21500
    }
)

# Query by strike range
redis_client.zrangebyscore(f"iv_surface:{product}", 21000, 22000)
```

**Use cases:**
- Leaderboards (score = points)
- Time-series (score = timestamp)
- Volatility surfaces (score = strike price)

**Why sorted sets for IV:**
- Efficient range queries (strikes between X and Y)
- Automatically sorted by strike
- O(log N) insert/lookup

---

### **13. WEBSOCKET (Flask-SocketIO)**

**What it is:**
Full-duplex communication protocol allowing bi-directional real-time data between client and server over a single TCP connection.

**HTTP vs WebSocket:**
```
HTTP (Request-Response):
Client â†’ Request â†’ Server
Client â† Response â† Server
(Need new request for updates)

WebSocket (Persistent Connection):
Client â†â†’ Server
(Server can push updates anytime)
```

**In your code:**
```python
# Server pushes data to client
socketio.emit('underlying_update', data, room='product:NIFTY')

# Client receives instantly (no polling needed)
socket.on('underlying_update', (data) => {
    console.log('Live price:', data.price);
});
```

**Why WebSocket for trading:**
- Real-time price updates (no polling)
- Low latency (<100ms)
- Efficient (single connection vs repeated HTTP requests)

---

### **14. ROOM-BASED SUBSCRIPTIONS**

**What it is:**
Socket.IO feature allowing clients to join "rooms" and receive targeted broadcasts.

**In your code:**
```python
# Client joins room
@socketio.on('subscribe')
def handle_subscribe(data):
    symbol = data['symbol']
    room = f"product:{symbol}"
    join_room(room)  # Client now in 'product:NIFTY' room

# Server broadcasts to specific room
socketio.emit('underlying_update', data, room='product:NIFTY')
# Only clients in 'product:NIFTY' room receive this
```

**Your room structure:**
```
'general' â†’ All clients (auto-joined)
'product:NIFTY' â†’ Only NIFTY subscribers
'product:BANKNIFTY' â†’ Only BANKNIFTY subscribers
'chain:NIFTY' â†’ NIFTY option chain subscribers
```

**Benefits:**
- **Bandwidth efficiency**: Clients receive only what they subscribed to
- **Scalability**: Don't broadcast everything to everyone
- **Flexibility**: Clients control what data they receive

---

### **15. REDIS MESSAGE QUEUE (for WebSocket)**

**What it is:**
Using Redis to coordinate multiple Socket.IO server instances so they can share connections.

**Problem without it:**
```
Client connects to Server 1
Data arrives at Server 2
Server 2 can't push to client (connected to Server 1)
```

**Solution with Redis message queue:**
```python
socketio = SocketIO(
    app,
    message_queue=redis_url  # All servers share via Redis
)

# Server 2 emits
socketio.emit('update', data, room='product:NIFTY')
â†“
Redis broadcasts to all Socket.IO servers
â†“
Server 1 receives and pushes to its connected clients
```

**Enables:**
- **Horizontal scaling**: Run multiple Socket.IO instances
- **Load balancing**: Distribute clients across servers
- **High availability**: If one server dies, others continue

---

### **16. MULTI-INSTANCE COORDINATION**

**What it is:**
Multiple instances of same service running simultaneously, coordinated through shared state (Redis).

**In your project:**
```
Socket Gateway Instance 1 â†â†’ Redis â†â†’ Socket Gateway Instance 2
      (Clients A, B)                        (Clients C, D)
```

**Coordination mechanisms:**
1. **Redis message queue**: Share WebSocket events
2. **Redis pub/sub**: Share data updates
3. **Redis cache**: Share state (connected clients, room memberships)

**Why coordinate:**
- Load balancing across instances
- No single point of failure
- Session persistence (client can reconnect to any instance)

---

### **17. KUBERNETES HPA (Horizontal Pod Autoscaler)**

**What it is:**
Kubernetes feature that automatically scales number of pods based on CPU/memory usage.

**In your manifest:**
```yaml
spec:
  minReplicas: 2        # Never fewer than 2
  maxReplicas: 10       # Never more than 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 70  # Scale up if CPU > 70%
```

**How it works:**
```
Normal load: 2 replicas running
â†“
Load increases, CPU hits 75%
â†“
HPA creates 1 more replica (now 3 total)
â†“
Load increases further, CPU still high
â†“
HPA creates more replicas (up to 10 max)
â†“
Load decreases, CPU drops below 70%
â†“
HPA removes replicas (down to 2 min)
```

**Why HPA:**
- **Cost efficiency**: Pay only for what you need
- **Automatic**: No manual intervention
- **Handles spikes**: Black swan events, market crashes â†’ auto-scale

---

### **18. DOCKER CONTAINERIZATION**

**What it is:**
Packaging application with all dependencies into isolated, portable containers.

**Your Dockerfile structure:**
```dockerfile
FROM python:3.10
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
```

**Benefits:**
- **Consistency**: Same environment dev â†’ staging â†’ prod
- **Isolation**: Each service in own container
- **Portability**: Runs anywhere (local, AWS, GCP, Azure)
- **Microservices**: Each service = separate container

---

### **19. MONGODB COMPOUND INDEXES**

**What it is:**
Index on multiple fields to optimize queries filtering/sorting by those fields.

**In your code:**
```python
# Compound index on (product + timestamp)
db.underlying_ticks.create_index([
    ('product', ASCENDING),
    ('timestamp', DESCENDING)
])
```

**Query optimization:**
```python
# This query uses the compound index efficiently
db.underlying_ticks.find({
    'product': 'NIFTY',
    'timestamp': {'$gte': start_time}
}).sort('timestamp', -1)

# Without index: Full collection scan (slow)
# With index: Index seek (fast)
```

**Your 3 indexed collections:**
1. `underlying_ticks`: (product, timestamp)
2. `option_quotes`: (product, timestamp) and (symbol, timestamp)
3. `option_chains`: (product, expiry, timestamp)

**Why compound over single:**
- Single index on `product`: Can filter by product, but sort is slow
- Compound index on `(product, timestamp)`: Fast filter + fast sort

---

### **20. STRUCTURED JSON LOGGING**

**What it is:**
Logging in JSON format with structured fields rather than plain text.

**Traditional logging:**
```
ERROR - Failed to process tick for NIFTY at 2025-01-15 10:30:00
```

**Structured JSON logging:**
```json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "error",
  "service": "worker-enricher",
  "event": "tick_processing_failed",
  "product": "NIFTY",
  "tick_id": 12345,
  "error": "MongoDB connection timeout",
  "trace_id": "abc123"
}
```

**In your code:**
```python
import structlog

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)
logger = structlog.get_logger()

logger.error(
    "tick_processing_failed",
    product="NIFTY",
    tick_id=12345,
    error=str(e)
)
```

**Benefits:**
- **Searchable**: Query logs by any field (show me all NIFTY errors)
- **Aggregatable**: Count errors by product, service, etc.
- **Machine-readable**: Easy to parse by log aggregators (ELK, Splunk)
- **Contextual**: Include request ID, user ID, trace ID

---

### **21. PROMETHEUS METRICS**

**What it is:**
Time-series monitoring system that scrapes metrics from services.

**Typical metrics in your services:**
```python
# Counter (always increasing)
requests_total{service="api-gateway", endpoint="/api/data/products"} 1523

# Gauge (can go up/down)
connected_websocket_clients{service="socket-gateway"} 47

# Histogram (distributions)
request_duration_seconds{service="storage"} 0.025
```

**In your code:**
```python
@app.route('/metrics')
def metrics():
    return {
        'total_clients': len(connected_clients),
        'rooms': room_counts,
        'messages_sent': message_counter
    }
```

**Why Prometheus:**
- Real-time monitoring
- Alerting (PagerDuty if CPU > 80%)
- Grafana dashboards
- Trend analysis

---

### **22. OBSERVABILITY STACK**

**What it is:**
Combination of logging, metrics, and tracing to understand system behavior.

**Three pillars:**

**1. Logs (Structured JSON)**
- What happened: "Order processed"
- When: "2025-01-15T10:30:00Z"
- Context: product, user_id, error details

**2. Metrics (Prometheus)**
- How many: Requests per second
- How fast: Latency percentiles (p50, p95, p99)
- How much: CPU, memory, disk usage

**3. Traces (Optional - Jaeger/Zipkin)**
- Request flow across services
- Latency breakdown per service
- Bottleneck identification

**Your observability:**
- Structured logging (structlog)
- Metrics endpoints (/health, /metrics)
- Health checks (MongoDB ping, Redis ping)

---

### **23. PCR (PUT-CALL RATIO)**

**What it is:**
Trading metric comparing put option volume/OI to call option volume/OI.

**Calculation:**
```python
total_call_oi = sum(c['open_interest'] for c in calls)
total_put_oi = sum(p['open_interest'] for p in puts)
pcr = total_put_oi / total_call_oi
```

**Interpretation:**
- PCR > 1.0: More puts than calls (bearish sentiment)
- PCR < 1.0: More calls than puts (bullish sentiment)
- PCR = 0.7-1.0: Typical neutral range

**In trading:**
- Contrarian indicator (high PCR = possible reversal)
- Sentiment gauge (institutional vs retail)

---

### **24. MAX PAIN**

**What it is:**
Strike price where option writers (sellers) profit the most, and option buyers lose the most.

**Calculation:**
```python
def calculate_max_pain(calls, puts, strikes):
    max_pain = None
    min_total_value = float('inf')
    
    for strike in strikes:
        # Total value of all options at this strike
        call_value = sum(
            c['open_interest'] * max(0, strike - c['strike'])
            for c in calls
        )
        put_value = sum(
            p['open_interest'] * max(0, p['strike'] - strike)
            for p in puts
        )
        total_value = call_value + put_value
        
        if total_value < min_total_value:
            min_total_value = total_value
            max_pain = strike
    
    return max_pain
```

**Theory:**
- Market tends to gravitate toward max pain at expiry
- Option writers (market makers) hedge to push price there
- Useful for predicting expiry settlement

---

### **25. IMPLIED VOLATILITY (IV) SURFACE**

**What it is:**
3D representation of implied volatility across strikes and expiries.

**Dimensions:**
- X-axis: Strike price
- Y-axis: Time to expiry
- Z-axis: Implied volatility

**In your code:**
```python
surface = {
    'product': 'NIFTY',
    'expiries': [
        {
            'expiry': '2025-01-25',
            'strikes': [21000, 21500, 22000],
            'ivs': [0.25, 0.20, 0.23],  # Higher IV at extremes (smile)
        },
        {
            'expiry': '2025-02-28',
            'strikes': [21000, 21500, 22000],
            'ivs': [0.22, 0.18, 0.20],
        }
    ]
}
```

**Why it matters:**
- **Volatility smile**: OTM options have higher IV
- **Volatility skew**: Puts have higher IV than calls (fear premium)
- **Arbitrage**: Identify mispriced options
- **Risk management**: Understand portfolio volatility exposure

---

### **26. OPTION GREEKS**

**What they are:**
Measures of option price sensitivity to various factors.

**Delta:**
- Change in option price per $1 change in underlying
- Call delta: 0 to 1, Put delta: -1 to 0
- Delta = 0.5 means ATM (50% chance of expiring ITM)

**Gamma:**
- Change in delta per $1 change in underlying
- Highest at ATM
- Shows how delta changes (acceleration)

**Vega:**
- Change in option price per 1% change in IV
- Long options have positive vega (benefit from volatility increase)

**Theta:**
- Option value lost per day (time decay)
- Always negative for long options
- Accelerates near expiry

---

### **27. OHLC WINDOWS**

**What it is:**
Aggregating tick data into time-based candles.

**In your code:**
```python
def calculate_ohlc_window(product, window_minutes):
    # Get ticks from last N minutes
    start_time = datetime.now() - timedelta(minutes=window_minutes)
    ticks = db.underlying_ticks.find({
        'product': product,
        'timestamp': {'$gte': start_time}
    }).sort('timestamp', ASCENDING)
    
    prices = [t['price'] for t in ticks]
    ohlc = {
        'open': prices[0],      # First price
        'high': max(prices),    # Highest price
        'low': min(prices),     # Lowest price
        'close': prices[-1],    # Last price
    }
    return ohlc
```

**Your windows:**
- 1-minute: High-frequency trading, scalping
- 5-minute: Intraday trading, technical analysis
- 15-minute: Swing trading, trend identification

---

## ðŸŽ¯ PART 2: COMPREHENSIVE INTERVIEW QUESTIONS & ANSWERS

---

## ðŸ“š **SECTION 1: MICROSERVICES ARCHITECTURE**

---

### **Q1: Explain your microservices architecture. Why did you choose this design?**

**Answer:**
"We built an 8-service architecture for a real-time trading analytics platform. Let me walk through the design:

**Services:**
1. **API Gateway** (Port 8000): Single entry point, routes requests to appropriate services
2. **Auth Service** (Port 8001): JWT-based authentication, user management
3. **Socket Gateway** (Port 8002): WebSocket server for real-time updates
4. **Storage Service** (Port 8003): MongoDB abstraction layer, data access
5. **Analytics Service** (Port 8004): PCR, Max Pain, IV surface calculations
6. **Worker Enricher**: Celery workers processing market data
7. **Feed Generator**: Simulates market data feeds
8. **Logging Service** (Port 8005): Centralized logging

**Why microservices:**
- **Independent scaling**: Workers process heavy analytics, so we can scale them (2-10 replicas) without scaling the API Gateway
- **Technology flexibility**: Could use Python for workers but Node.js for WebSocket if needed
- **Fault isolation**: If Analytics service crashes, real-time data streaming continues
- **Team autonomy**: Different developers can own different services
- **Deployment**: Can deploy Analytics changes without touching Auth

**Communication:**
- Synchronous: HTTP REST between API Gateway and other services
- Asynchronous: Redis Pub/Sub for event-driven data flow

This design allowed us to process real-time market data with <100ms latency while maintaining 99%+ uptime."

**Follow-up Q: What challenges did you face with microservices?**

"Three main challenges:

1. **Distributed debugging**: When a request fails, need to trace across 3-4 services. Solution: Implemented structured JSON logging with trace IDs

2. **Data consistency**: No ACID transactions across services. Solution: Used event sourcing with idempotency to achieve eventual consistency

3. **Service discovery**: Services need to find each other. Solution: Used Docker Compose networking (service names as DNS) for local, would use Kubernetes services in production

4. **Network latency**: Inter-service calls add latency. Solution: Cached frequently accessed data in Redis with 5min TTL"

---

### **Q2: How do services communicate? Walk me through a request flow.**

**Answer:**
"We use two communication patterns:

**Pattern 1: Synchronous (REST API)**
Used when client needs immediate response.

Example: User requests NIFTY option chain
```
Client â†’ API Gateway (8000) â†’ Storage Service (8003) â†’ MongoDB
       â† JSON Response  â†
```

**Pattern 2: Asynchronous (Redis Pub/Sub)**
Used for real-time data flow.

Example: Market data processing
```
Feed Generator â†’ Redis Pub/Sub â†’ Worker Enricher â†’ MongoDB + Redis Cache
                 (market:underlying)    (Celery)     (Storage + Cache)
                                            â†“
                                    Redis Pub/Sub â†’ Socket Gateway â†’ WebSocket Clients
                                 (enriched:underlying)   (8002)
```

**Detailed flow for option chain:**
1. Feed Generator publishes to `market:option_chain` channel
2. Worker Enricher subscriber receives message
3. Dispatches Celery task `process_option_chain.delay(data)`
4. Worker:
   - Calculates PCR, Max Pain, Greeks
   - Stores in MongoDB
   - Updates Redis cache with 5min TTL
   - Publishes to `enriched:option_chain`
5. Socket Gateway receives enriched data
6. Broadcasts to subscribed WebSocket clients in real-time

Total latency: ~50-100ms from feed to client"

**Follow-up Q: Why use both patterns? Why not just REST everywhere?**

"Great question. Each has its use case:

**REST (Synchronous):**
- âœ… Client needs immediate response (API requests)
- âœ… Strong consistency required
- âœ… Simple request-response
- âŒ Blocks caller, coupling services

**Pub/Sub (Asynchronous):**
- âœ… Fire-and-forget (no response needed)
- âœ… Multiple consumers (Worker, Socket Gateway both listen)
- âœ… Loose coupling (publisher doesn't know subscribers)
- âœ… High throughput (non-blocking)
- âŒ No immediate response
- âŒ Eventual consistency

For our trading platform:
- User queries (Get option chain) â†’ REST (need data immediately)
- Market data processing â†’ Pub/Sub (high throughput, multiple consumers)

This hybrid approach gives us best of both worlds."

---

### **Q3: What happens if one service goes down?**

**Answer:**
"We designed for fault tolerance at multiple levels:

**Scenario 1: Worker Enricher crashes**
- Redis Pub/Sub buffers messages (up to memory limit)
- When worker restarts, catches up on backlog
- Celery's `acks_late=True` ensures no message loss
- DLQ catches poison messages

**Scenario 2: Storage Service crashes**
- API Gateway returns 503 Service Unavailable
- Circuit breaker pattern (could implement) prevents cascade failure
- MongoDB replica set ensures data persistence
- Recent data still available from Redis cache

**Scenario 3: Socket Gateway crashes**
- Clients automatically reconnect (Socket.IO built-in)
- Redis message queue allows other Socket Gateway instances to serve clients
- No data loss (data in Redis/MongoDB)

**Scenario 4: Redis crashes**
- Biggest impact: Cache miss on all requests â†’ Higher DB load
- Pub/Sub messages lost (not persistent by default)
- Solution: Redis Sentinel/Cluster for HA, or Kafka for persistent messaging

**Mitigation strategies:**
1. **Health checks**: Each service exposes `/health` endpoint
2. **Kubernetes liveness probes**: Auto-restart unhealthy pods
3. **HPA**: Scale up healthy instances when others fail
4. **Retry logic**: Services retry failed requests with exponential backoff
5. **Graceful degradation**: Serve cached data if DB unavailable"

**Follow-up Q: How would you improve fault tolerance?**

"Three improvements:

1. **Circuit Breaker Pattern**: If Storage Service fails 5 times in 10s, API Gateway stops calling it for 30s, returns cached data

2. **Message persistence**: Replace Redis Pub/Sub with Kafka for durable messages (survives crashes)

3. **Database replication**: MongoDB replica set with read replicas (1 primary, 2 secondary) for high availability

4. **Rate limiting**: Prevent cascading failures from one overloaded service affecting others"

---

## ðŸ“š **SECTION 2: REDIS PUB/SUB & EVENT-DRIVEN ARCHITECTURE**

---

### **Q4: Explain how Redis Pub/Sub works in your system.**

**Answer:**
"Redis Pub/Sub is our message broker for event-driven data flow. Let me explain:

**Core Concept:**
Publishers send messages to channels. Subscribers listen to channels. Redis routes messages from publishers to all subscribers of that channel.

**Our implementation:**

**Publishers:**
- Feed Generator publishes market data
```python
redis_client.publish('market:underlying', json.dumps({
    'product': 'NIFTY',
    'price': 21543.25,
    'timestamp': '2025-01-15T10:30:00Z'
}))
```

**Subscribers:**
- Worker Enricher subscribes to process data
```python
pubsub = redis_client.pubsub()
pubsub.subscribe('market:underlying', 'market:option_chain')

for message in pubsub.listen():
    if message['type'] == 'message':
        data = json.loads(message['data'])
        if message['channel'] == 'market:underlying':
            process_underlying_tick.delay(data)
```

**Our 5 channels:**
1. `market:underlying` - Raw price ticks
2. `market:option_quote` - Individual option quotes
3. `market:option_chain` - Complete option chains
4. `enriched:underlying` - Processed with OHLC, cached
5. `enriched:option_chain` - With PCR, Max Pain, Greeks

**Data flow:**
```
Feed â†’ market:* â†’ Worker â†’ enriched:* â†’ Socket Gateway â†’ Clients
```

**Benefits:**
- Decoupled: Feed doesn't know who consumes data
- Multi-consumer: Worker AND Logger both subscribe
- Asynchronous: Feed doesn't wait for processing
- Scalable: Add more subscribers without changing publisher"

**Follow-up Q: What are limitations of Redis Pub/Sub?**

"Great question. Redis Pub/Sub has significant limitations:

**1. No persistence:**
- If subscriber is offline, messages are lost
- No replay capability

**2. No acknowledgments:**
- Fire-and-forget, no confirmation of delivery
- If consumer crashes mid-processing, message lost

**3. No backpressure:**
- If subscriber is slow, messages buffer in Redis memory
- Can cause OOM if not careful

**4. At-most-once delivery:**
- Message delivered once or not at all
- No exactly-once guarantees

**Solutions:**
- For critical data: Use Celery (Redis as message broker) instead - has persistence, retries, acks
- For durability: Use Kafka - persistent log, replay capability
- For our use case: Acceptable because data is high-frequency and ephemeral (ticks), and we store in MongoDB for persistence

**Why we still use Pub/Sub:**
- Real-time data doesn't need persistence (new tick every second)
- Data already stored in MongoDB for history
- Simpler than Kafka for our scale
- Fast (< 1ms latency)"

---

### **Q5: What's the difference between Redis Pub/Sub and a message queue like RabbitMQ or Kafka?**

**Answer:**

**Redis Pub/Sub:**
- **Delivery**: Broadcasts to ALL subscribers (1-to-many)
- **Persistence**: No, messages lost if no subscriber
- **Acknowledgment**: No
- **Use case**: Real-time broadcasts (chat, live updates)

**Message Queue (RabbitMQ/Celery):**
- **Delivery**: ONE consumer processes message (1-to-1)
- **Persistence**: Yes, messages stored until consumed
- **Acknowledgment**: Yes, consumer acknowledges processing
- **Use case**: Task distribution (send email, process payment)

**Kafka:**
- **Delivery**: Consumer groups (each group gets copy, within group only one consumer)
- **Persistence**: Yes, durable log for days/weeks
- **Acknowledgment**: Yes, offset-based
- **Replay**: Can re-consume from any point
- **Use case**: Event sourcing, audit logs, data pipelines

**In our system:**
- **Redis Pub/Sub**: Market data broadcasting (Feed â†’ Workers)
- **Celery (Redis queue)**: Task processing (distribute work across workers)
- **MongoDB**: Persistence layer

**Example:**
```
Redis Pub/Sub:
Feed publishes 1 message â†’ Worker 1 receives it, Worker 2 receives it (both process)

Celery Queue:
Feed dispatches 1 task â†’ Worker 1 OR Worker 2 processes it (not both)
```

If I were to scale this for production, I'd replace Redis Pub/Sub with Kafka for durability and replay capability, but keep Celery for task distribution."

**Follow-up Q: When would you choose Kafka over Redis Pub/Sub?**

"I'd choose Kafka when:

1. **Data durability matters**: Financial transactions, audit logs, billing events
2. **Need to replay**: Debugging, reprocessing data, recovering from bugs
3. **High throughput**: 100K+ msg/sec (Kafka can handle millions)
4. **Multiple consumer groups**: Team A and Team B both need full stream
5. **Long-term storage**: Keep events for 30 days for analysis

I'd stick with Redis Pub/Sub when:
1. **Real-time, ephemeral**: Live scores, chat messages, stock ticks
2. **Simplicity**: Don't need Kafka's operational complexity
3. **Low latency**: <1ms latency (Kafka is ~10ms)
4. **Prototyping**: Faster to set up

For our trading platform, Redis Pub/Sub works because:
- Market ticks are ephemeral (new data every second)
- We persist in MongoDB anyway
- Low latency requirement (<100ms)
- Simpler ops (no Kafka cluster to manage)"

---

## ðŸ“š **SECTION 3: CELERY & ASYNC PROCESSING**

---

### **Q6: Explain how Celery works in your system. Walk me through task dispatch to completion.**

**Answer:**
"Celery is our distributed task queue for asynchronous processing. Here's the full flow:

**Architecture:**
```
Producer (Your App) â†’ Broker (Redis) â†’ Worker (Celery) â†’ Result Backend (Redis)
```

**Step-by-step for processing an option chain:**

**1. Task Definition:**
```python
@celery_app.task(base=EnrichmentTask, bind=True)
def process_option_chain(self, chain_data):
    # Calculate PCR
    # Calculate Max Pain
    # Store in MongoDB
    # Update cache
```

**2. Task Dispatch (Producer):**
```python
# Non-blocking call, returns immediately
task = process_option_chain.delay(chain_data)
print(f"Task ID: {task.id}")  # abc-123-def
# Code continues without waiting
```

**3. Message in Broker:**
Celery serializes task to JSON and pushes to Redis:
```json
{
  "id": "abc-123-def",
  "task": "app.process_option_chain",
  "args": [{"product": "NIFTY", "calls": [...], "puts": [...]}],
  "kwargs": {},
  "retries": 0
}
```

**4. Worker Picks Up Task:**
- Worker polls Redis broker
- Retrieves task message
- Deserializes JSON
- Executes `process_option_chain(chain_data)`

**5. Task Execution:**
```python
def process_option_chain(chain_data):
    # 1. Calculate analytics
    pcr = calculate_pcr(chain_data)
    max_pain = calculate_max_pain(chain_data)
    
    # 2. Store in MongoDB
    db.option_chains.insert_one(enriched_data)
    
    # 3. Update Redis cache
    redis_client.setex(f"latest:chain:{product}:{expiry}", 300, json.dumps(data))
    
    # 4. Publish enriched data
    redis_client.publish('enriched:option_chain', json.dumps(data))
    
    return {"status": "success", "pcr": pcr}
```

**6. Result Storage:**
- Worker stores result in Result Backend (Redis)
- Producer can retrieve: `result = task.get()`

**7. Task Acknowledgment:**
- With `acks_late=True`, worker ACKs only after successful completion
- If worker crashes mid-task, message stays in queue for retry

**Concurrency:**
- We run multiple workers (HPA: 2-10 replicas)
- Each worker processes one task at a time (`prefetch_multiplier=1`)
- Redis distributes tasks evenly across workers

This architecture allows us to process hundreds of option chains per second without blocking the API."

**Follow-up Q: What happens if the task fails?**

"We have a comprehensive retry and error handling strategy:

**1. Automatic Retry:**
```python
celery_app.conf.update(
    task_autoretry_for=(Exception,),
    task_retry_kwargs={'max_retries': 3, 'countdown': 5}
)
```

**Retry timeline:**
```
Attempt 1: Task fails (MongoDB timeout)
            â†“ Wait 5 seconds
Attempt 2: Task fails again
            â†“ Wait 10 seconds (exponential backoff)
Attempt 3: Task fails again
            â†“ Wait 20 seconds
Attempt 4: Task fails again
            â†“ Max retries exceeded
            â†“ Call on_failure() hook
```

**2. Dead Letter Queue:**
```python
def on_failure(self, exc, task_id, args, kwargs, einfo):
    dlq_message = {
        'task_id': task_id,
        'error': str(exc),
        'args': args,
        'timestamp': datetime.now().isoformat()
    }
    redis_client.lpush('dlq:enrichment', json.dumps(dlq_message))
    
    # Alert ops team
    send_alert(f"Task {task_id} failed after 3 retries")
```

**3. Monitoring DLQ:**
```bash
# Check DLQ size
redis-cli LLEN dlq:enrichment

# View failed tasks
redis-cli LRANGE dlq:enrichment 0 -1
```

**4. Manual Replay:**
```python
# Ops can manually reprocess
dlq_messages = redis_client.lrange('dlq:enrichment', 0, -1)
for msg in dlq_messages:
    task_data = json.loads(msg)
    process_option_chain.delay(*task_data['args'])
```

This ensures no data is silently lost, we're alerted to systemic issues, and we can recover from failures."

---

### **Q7: Explain exponential backoff. Why is it better than fixed retry?**

**Answer:**
"Exponential backoff means retry delays increase exponentially: 5s â†’ 10s â†’ 20s â†’ 40s.

**Our implementation:**
```python
task_retry_kwargs={'max_retries': 3, 'countdown': 5}
```

**Why exponential is better:**

**Scenario: MongoDB is temporarily overloaded**

**Fixed Retry (every 5s):**
```
Task 1 fails â†’ Retry after 5s â†’ Fails again â†’ Retry after 5s â†’ Fails
Task 2 fails â†’ Retry after 5s â†’ Fails again â†’ Retry after 5s â†’ Fails
Task 3 fails â†’ Retry after 5s â†’ Fails again â†’ Retry after 5s â†’ Fails
...
Result: 1000 tasks all retrying every 5s â†’ MongoDB gets HAMMERED
```

**Exponential Backoff:**
```
Task 1 fails â†’ Retry after 5s â†’ Fails â†’ Retry after 10s â†’ Succeeds (DB recovered)
Task 2 fails â†’ Retry after 5s â†’ Succeeds
Task 3 fails â†’ Retry after 5s â†’ Succeeds
...
Result: Gives MongoDB breathing room to recover
```

**Benefits:**
1. **Prevents thundering herd**: Not all tasks retry simultaneously
2. **Self-healing**: System has time to recover (connection pool refills, memory clears)
3. **Resource efficiency**: Fewer wasted retry attempts

**Real-world causes of transient failures:**
- Network hiccups (packet loss)
- Database connection pool exhaustion
- Temporary CPU spike
- Brief network partition
- Lock contention in MongoDB

Exponential backoff is industry standard for handling transient failures (used by AWS SDKs, Google APIs, etc.)."

**Follow-up Q: What's the maximum retry delay in your system?**

"Great question. Let me calculate:

```
Attempt 1: 5s delay
Attempt 2: 5s * 2^1 = 10s delay
Attempt 3: 5s * 2^2 = 20s delay
Total delay: 5 + 10 + 20 = 35 seconds before giving up
```

**Considerations for delay tuning:**

**Too short (3 retries in 15s):**
- System hasn't recovered yet
- Still overwhelms failing service
- More DLQ entries

**Too long (3 retries in 10 minutes):**
- Data freshness suffers
- Users see stale data
- Real issues take longer to detect

**Our 35s is good because:**
- Most transient issues resolve in <30s
- Trading data doesn't need to be >1min old
- If still failing after 35s, likely systemic (needs manual intervention)

**We could improve with:**
- Jitter: Random Â±20% to prevent synchronization
```python
import random
delay = base_delay * (2 ** attempt) * (1 + random.uniform(-0.2, 0.2))
```

- Max cap: Never wait more than 60s
```python
delay = min(base_delay * (2 ** attempt), 60)
```"

---

### **Q8: What is task idempotency and why is it critical?**

**Answer:**
"Idempotency means a task can be executed multiple times but produces the same result as executing it once.

**Why we need it:**

**Scenario without idempotency:**
```
1. Task processes NIFTY tick (tick_id=123)
2. Worker stores in MongoDB â†’ Success
3. Worker crashes before ACKing
4. Celery retries task (because no ACK)
5. Task processes NIFTY tick (tick_id=123) AGAIN
6. MongoDB has duplicate entry

Result: Double-counting in analytics, incorrect PCR
```

**Our implementation:**
```python
def process_underlying_tick(tick_data):
    tick_id = tick_data['tick_id']
    product = tick_data['product']
    
    # Idempotency check
    idempotency_key = f"processed:underlying:{product}:{tick_id}"
    if redis_client.exists(idempotency_key):
        logger.info("Already processed, skipping", tick_id=tick_id)
        return  # Safe to return, no side effects
    
    # Process the tick
    db.underlying_ticks.insert_one(tick_data)
    redis_client.setex(f"latest:underlying:{product}", 300, json.dumps(tick_data))
    
    # Mark as processed
    redis_client.setex(idempotency_key, 3600, '1')  # 1 hour TTL
```

**How it works:**
1. Before processing, check Redis: "Have I processed tick_id=123?"
2. If YES â†’ Skip (already done)
3. If NO â†’ Process and mark as processed

**TTL (1 hour):**
- After 1 hour, idempotency key expires
- Trade-off: Memory vs safety window
- 1 hour is sufficient because retries happen within seconds

**Why critical for distributed systems:**
- Network failures can cause duplicate messages
- Celery retries can reprocess same task
- Load balancers can duplicate requests
- Message brokers may deliver at-least-once

**At-least-once + Idempotency = Exactly-once semantics**"

**Follow-up Q: What if Redis goes down? Your idempotency checks fail.**

"You're right, that's a vulnerability. If Redis is down, idempotency checks fail-open, and we might process duplicates.

**Better solutions:**

**1. Database-level idempotency:**
```python
db.underlying_ticks.insert_one({
    'tick_id': tick_data['tick_id'],  # Unique index on tick_id
    'product': tick_data['product'],
    'price': tick_data['price']
})
# If duplicate, MongoDB raises DuplicateKeyError â†’ catch and skip
```

**2. Unique constraints:**
```python
db.underlying_ticks.create_index([('product', 1), ('tick_id', 1)], unique=True)
```

**3. Application-level state machine:**
```python
# Store processing state in DB
status = db.tick_processing.find_one({'tick_id': tick_id})
if status and status['state'] == 'completed':
    return

# Atomically update state
db.tick_processing.update_one(
    {'tick_id': tick_id},
    {'$set': {'state': 'processing'}},
    upsert=True
)

# Process...

db.tick_processing.update_one(
    {'tick_id': tick_id},
    {'$set': {'state': 'completed'}}
)
```

**Our trade-off:**
- Redis is faster than MongoDB (1ms vs 10ms)
- Redis is less critical (if down, we can tolerate duplicates briefly)
- For production, I'd use MongoDB unique constraints as backup

**Defense in depth:**
1. Redis idempotency (fast path)
2. MongoDB unique constraints (fallback)
3. Monitoring DLQ for duplicate errors"

---

### **Q9: Explain `acks_late=True`. Why is it important?**

**Answer:**
"`acks_late` controls when Celery acknowledges a task to the broker.

**Two modes:**

**Default (`acks_late=False` - Early ACK):**
```
1. Worker receives task from Redis
2. Worker sends ACK to Redis immediately â† ACK sent
3. Worker starts processing task
4. [Worker crashes during processing]
5. Task is LOST (already ACKed)
```

**Our setting (`acks_late=True` - Late ACK):**
```
1. Worker receives task from Redis
2. Worker starts processing task
3. Worker processes task (calculate PCR, store in DB)
4. Worker completes task successfully
5. Worker sends ACK to Redis â† ACK sent only after completion
```

**Why late ACK is critical:**

**Scenario: Worker crashes mid-processing**
```
With early ACK:
Task received â†’ ACK sent â†’ Processing... â†’ [CRASH]
Result: Task lost forever, data never processed

With late ACK:
Task received â†’ Processing... â†’ [CRASH] â†’ No ACK sent
Result: Redis still has task in queue â†’ Another worker picks it up
```

**Our implementation:**
```python
celery_app.conf.update(
    task_acks_late=True,                    # Don't ACK until done
    worker_prefetch_multiplier=1,           # One task at a time
    task_reject_on_worker_lost=True,        # Return to queue if crash
)
```

**Trade-off:**
- **Advantage**: No message loss on crash
- **Disadvantage**: If worker crashes, task is retried (need idempotency)

**Combined with idempotency:**
```
Task A processed â†’ Worker crashes before ACK â†’ Task A re-queued
â†’ New worker picks up â†’ Idempotency check: "Already processed" â†’ Skip
Result: No data loss, no duplicates
```

This is standard for at-least-once delivery in distributed systems."

**Follow-up Q: What if the task is slow? Could it timeout?**

"Yes, that's a risk. If a task runs longer than the broker's visibility timeout, the broker might think the worker is dead and re-queue the task.

**Celery's handling:**
- Celery sends heartbeats to broker while task is running
- Broker knows worker is alive and processing
- No timeout as long as heartbeats continue

**Problem cases:**
1. **Network partition**: Worker can't send heartbeats
2. **Infinite loop**: Task never completes
3. **Deadlock**: Task waiting on resource forever

**Solutions:**

**1. Task timeouts:**
```python
@celery_app.task(time_limit=300)  # Kill task after 5 minutes
def process_option_chain(data):
    ...
```

**2. Soft timeouts (graceful):**
```python
@celery_app.task(soft_time_limit=270, time_limit=300)
def process_option_chain(data):
    try:
        # Processing
    except SoftTimeLimitExceeded:
        # Cleanup, save partial results
        raise
```

**3. Monitoring:**
```python
# Alert if task takes >5 minutes
if task_duration > 300:
    send_alert(f"Task {task_id} slow: {task_duration}s")
```

**For our use case:**
- Most tasks complete in <100ms (simple analytics)
- Set timeout at 60s (60x expected duration)
- If task hits timeout, likely bug or data issue â†’ DLQ + investigate"

---

## ðŸ“š **SECTION 4: REDIS CACHING**

---

### **Q10: Explain your Redis caching strategy. Why multi-layer?**

**Answer:**
"We use a 3-level caching architecture with different TTLs and access patterns.

**Level 1: Hot Data (Real-time, 5min TTL)**
```python
latest:underlying:{product}      # Latest price for NIFTY
latest:option:{symbol}           # Latest quote for specific option
latest:chain:{product}:{expiry}  # Latest full option chain
latest:pcr:{product}:{expiry}    # Latest PCR values
```

**Level 2: Computed Data (Analytics, varies)**
```python
ohlc:{product}:{window}m         # OHLC for 1/5/15 min windows
volatility_surface:{product}     # IV surface (5min TTL)
```

**Level 3: Operational (1hr TTL)**
```python
processed:underlying:{product}:{tick_id}  # Idempotency tracking
dlq:enrichment                             # Dead letter queue
```

**Why multi-layer:**

**1. Different freshness requirements:**
- Latest prices: Update every second, 5min TTL okay
- OHLC windows: Recalculate every window (5min OHLC updates every 5min)
- Idempotency: Only needs to persist for retry window (1 hour)

**2. Memory optimization:**
- Hot data: Small keys, frequent access, short TTL â†’ Auto-cleanup
- Cold data: Longer TTL, less frequent cleanup

**3. Access patterns:**
- Level 1: Read-heavy (thousands of reads/sec)
- Level 2: Compute-heavy (expensive to regenerate)
- Level 3: Write-heavy (idempotency checks)

**Cache-aside implementation:**
```python
def get_underlying_price(product):
    # Try Level 1 cache
    cached = redis_client.get(f"latest:underlying:{product}")
    if cached:
        return json.loads(cached)  # Cache HIT
    
    # Cache MISS - query MongoDB
    data = db.underlying_ticks.find_one(
        {'product': product},
        sort=[('timestamp', -1)]
    )
    
    # Update cache with TTL
    redis_client.setex(
        f"latest:underlying:{product}",
        300,  # 5 minutes
        json.dumps(data)
    )
    
    return data
```

**Benefits:**
- Sub-50ms response time on cache hits
- Reduces MongoDB load (estimated 60% fewer queries)
- Auto-expiration prevents stale data"

**Follow-up Q: What's your cache hit ratio? How do you measure it?**

"We don't have production metrics (project not deployed), but here's how I'd measure:

**1. Instrument cache operations:**
```python
cache_hits = 0
cache_misses = 0

def get_with_metrics(key):
    global cache_hits, cache_misses
    
    cached = redis_client.get(key)
    if cached:
        cache_hits += 1
        return json.loads(cached)
    else:
        cache_misses += 1
        # Fetch from DB and cache
        ...
```

**2. Expose metrics endpoint:**
```python
@app.route('/metrics')
def metrics():
    hit_ratio = cache_hits / (cache_hits + cache_misses)
    return {
        'cache_hits': cache_hits,
        'cache_misses': cache_misses,
        'hit_ratio': hit_ratio
    }
```

**3. Expected hit ratios:**
- **Level 1 (latest prices)**: 90-95% (frequently requested)
- **Level 2 (OHLC)**: 70-80% (computed data, less frequent)
- **Level 3 (idempotency)**: 5-10% (mostly writes, few duplicate checks)

**4. Optimization if hit ratio low:**
- Increase TTL (trade-off with freshness)
- Pre-warm cache (populate before requests)
- Predictive caching (fetch what user likely needs next)

**5. Prometheus metrics:**
```python
from prometheus_client import Counter, Histogram

cache_hit_counter = Counter('cache_hits_total', 'Total cache hits')
cache_miss_counter = Counter('cache_misses_total', 'Total cache misses')
cache_latency = Histogram('cache_latency_seconds', 'Cache operation latency')

@cache_latency.time()
def get_with_metrics(key):
    cached = redis_client.get(key)
    if cached:
        cache_hit_counter.inc()
    else:
        cache_miss_counter.inc()
    ...
```

This would let us visualize hit ratio over time in Grafana."

---

### **Q11: How do you handle cache invalidation?**

**Answer:**
"Cache invalidation is 'one of the two hard problems in computer science.' We use TTL-based expiration with event-driven invalidation.

**Strategy 1: TTL-based (Lazy invalidation)**
```python
redis_client.setex(key, 300, value)  # Auto-expires after 5 min
```

**Benefits:**
- Simple, automatic cleanup
- No manual invalidation logic
- Memory efficient

**Drawback:**
- Stale data for up to 5 minutes

**Strategy 2: Event-driven (Active invalidation)**
```python
def process_option_chain(chain_data):
    product = chain_data['product']
    expiry = chain_data['expiry']
    
    # Process and calculate
    enriched = calculate_analytics(chain_data)
    
    # Update MongoDB
    db.option_chains.insert_one(enriched)
    
    # Update cache with fresh data (invalidate by overwriting)
    redis_client.setex(
        f"latest:chain:{product}:{expiry}",
        300,
        json.dumps(enriched)
    )
```

**This is 'cache-aside with write-through invalidation':**
- Write to database
- Update cache immediately (invalidate old)
- Ensures cache is always fresh

**Strategy 3: Pattern-based invalidation**
```python
def invalidate_product_caches(product):
    # Delete all keys matching pattern
    pattern = f"*:{product}:*"
    keys = redis_client.keys(pattern)
    if keys:
        redis_client.delete(*keys)
```

**Use cases:**
- User updates product settings â†’ Invalidate all related caches
- System detects bad data â†’ Purge all caches

**Strategy 4: Cache versioning**
```python
CACHE_VERSION = 'v2'
redis_client.set(f"latest:underlying:{product}:{CACHE_VERSION}", data)

# On schema change, bump version â†’ Old caches auto-expire
CACHE_VERSION = 'v3'  # v2 caches now orphaned, will expire via TTL
```

**Our hybrid approach:**
- **Normal case**: TTL-based (5min) â†’ Simple
- **Data update**: Event-driven (immediate overwrite) â†’ Fast
- **Emergency**: Pattern-based deletion â†’ Manual intervention

**Trade-offs:**
- **Short TTL (1min)**: Fresh data, more DB load
- **Long TTL (1hr)**: Less DB load, stale data
- **Our 5min**: Balance for trading data (not critical to be <5min fresh)"

**Follow-up Q: What about cache stampede (thundering herd)?**

"Great question! Cache stampede happens when:
1. Popular cache key expires
2. 1000 requests arrive simultaneously
3. All 1000 find cache miss
4. All 1000 query database simultaneously
5. Database overwhelmed

**Our vulnerability:**
```python
cached = redis_client.get(f"latest:chain:{product}:{expiry}")
if not cached:
    # 1000 requests reach here simultaneously
    data = db.option_chains.find_one(...)  # 1000 DB queries!
    redis_client.setex(key, 300, json.dumps(data))
```

**Solution 1: Locking (pessimistic)**
```python
import redis_lock

lock = redis_lock.Lock(redis_client, f"lock:chain:{product}:{expiry}")
if lock.acquire(blocking=False):
    try:
        # Only ONE request queries DB
        data = db.option_chains.find_one(...)
        redis_client.setex(key, 300, json.dumps(data))
    finally:
        lock.release()
else:
    # Other requests wait for cache to be populated
    time.sleep(0.1)
    cached = redis_client.get(key)
```

**Solution 2: Probabilistic early expiration**
```python
import random

ttl = redis_client.ttl(key)
expiry_threshold = 300 * 0.1  # 10% of TTL (30 seconds)

if ttl < expiry_threshold and random.random() < 0.1:
    # 10% chance to refresh if TTL < 30s
    data = db.option_chains.find_one(...)
    redis_client.setex(key, 300, json.dumps(data))
```

**Solution 3: Background refresh**
```python
@celery_app.task
def refresh_cache_task(product, expiry):
    data = db.option_chains.find_one({'product': product, 'expiry': expiry})
    redis_client.setex(f"latest:chain:{product}:{expiry}", 300, json.dumps(data))

# Scheduled task refreshes popular caches before expiry
celery_app.conf.beat_schedule = {
    'refresh-nifty-cache': {
        'task': 'refresh_cache_task',
        'schedule': 240.0,  # Every 4 min (before 5min expiry)
        'args': ('NIFTY', '2025-01-25')
    }
}
```

**For production, I'd use Solution 1 (locking) for critical paths and Solution 3 (background refresh) for frequently accessed data."

---

## ðŸ“š **SECTION 5: WEBSOCKET & REAL-TIME DATA**

---

### **Q12: How does your WebSocket architecture work? How do you scale it?**

**Answer:**
"We use Flask-SocketIO for real-time data streaming with horizontal scaling via Redis message queue.

**Basic architecture:**
```python
# Initialize Socket.IO with Redis backend
socketio = SocketIO(
    app,
    cors_allowed_origins="*",
    message_queue=redis_url,  # Critical for scaling
    async_mode='threading'
)
```

**Client connection flow:**
```python
# 1. Client connects
@socketio.on('connect')
def handle_connect():
    client_id = request.sid
    join_room('general')  # Auto-join global room
    emit('connected', {'client_id': client_id})

# 2. Client subscribes to NIFTY
@socketio.on('subscribe')
def handle_subscribe(data):
    symbol = data['symbol']  # 'NIFTY'
    room = f"product:{symbol}"
    join_room(room)  # Client now in 'product:NIFTY' room
```

**Data broadcasting flow:**
```python
# Background thread listens to Redis Pub/Sub
def redis_listener():
    pubsub = redis_client.pubsub()
    pubsub.subscribe('enriched:underlying')
    
    for message in pubsub.listen():
        if message['type'] == 'message':
            data = json.loads(message['data'])
            product = data['product']
            
            # Broadcast to product-specific room
            socketio.emit('underlying_update', data, room=f"product:{product}")
```

**Horizontal scaling (Multiple instances):**

**Without Redis message queue:**
```
Instance 1 (Clients A, B) â†’ Receives data â†’ Broadcasts to A, B
Instance 2 (Clients C, D) â†’ Receives data â†’ Broadcasts to C, D

Problem: If data arrives at Instance 1, only A and B get it. C and D miss it.
```

**With Redis message queue:**
```
Instance 1 â†’ socketio.emit() â†’ Redis
                                 â†“
                        Broadcasts to all instances
                                 â†“
             Instance 1 â† Redis â†’ Instance 2
             (A, B get it)        (C, D get it)
```

**Implementation:**
```python
# Instance 1 emits
socketio.emit('underlying_update', data, room='product:NIFTY')

# Redis distributes to all Socket.IO instances
# All instances broadcast to their connected clients
```

**Room-based targeting:**
```
Client A subscribes to 'product:NIFTY'
Client B subscribes to 'product:BANKNIFTY'
Client C subscribes to 'product:NIFTY'

Broadcast to 'product:NIFTY' â†’ Only A and C receive
```

**Scaling in Kubernetes:**
```yaml
# Multiple Socket Gateway pods
replicas: 3

# Clients load-balanced across pods
# Redis message queue ensures consistency
```

This architecture allows us to scale to thousands of concurrent connections by adding more Socket Gateway instances."

**Follow-up Q: How do you handle reconnections?**

"Socket.IO has built-in reconnection logic, but we add application-level handling:

**Client-side:**
```javascript
const socket = io('http://localhost:8002', {
    reconnection: true,
    reconnectionDelay: 1000,      // Start at 1s
    reconnectionDelayMax: 5000,   // Max 5s
    reconnectionAttempts: 5
});

socket.on('connect', () => {
    // Re-subscribe after reconnection
    socket.emit('subscribe', {type: 'product', symbol: 'NIFTY'});
});

socket.on('disconnect', (reason) => {
    if (reason === 'io server disconnect') {
        // Server forced disconnect, manual reconnect
        socket.connect();
    }
    // Otherwise, auto-reconnect
});
```

**Server-side:**
```python
@socketio.on('connect')
def handle_connect():
    client_id = request.sid
    
    # Check if reconnecting client
    previous_session = redis_client.get(f"session:{client_id}")
    if previous_session:
        # Restore subscriptions
        subscriptions = json.loads(previous_session)
        for room in subscriptions['rooms']:
            join_room(room)
        
        emit('reconnected', {'restored_rooms': subscriptions['rooms']})
    else:
        # New connection
        join_room('general')
        emit('connected', {'client_id': client_id})

@socketio.on('disconnect')
def handle_disconnect():
    client_id = request.sid
    
    # Save session for potential reconnection
    session_data = {
        'rooms': list(rooms()),
        'timestamp': datetime.now().isoformat()
    }
    redis_client.setex(f"session:{client_id}", 300, json.dumps(session_data))
```

**Handling missed messages:**
```python
@socketio.on('subscribe')
def handle_subscribe(data):
    symbol = data['symbol']
    room = f"product:{symbol}"
    join_room(room)
    
    # Send latest cached data immediately
    cached = redis_client.get(f"latest:underlying:{symbol}")
    if cached:
        emit('underlying_update', json.loads(cached))
```

**Exponential backoff:**
1s â†’ 2s â†’ 4s â†’ 5s (max)

This ensures clients reconnect gracefully without overwhelming the server during outages."

---

### **Q13: What's the difference between WebSocket and HTTP polling? Why use WebSocket?**

**Answer:**

**HTTP Polling (Old way):**
```javascript
// Client polls every 1 second
setInterval(() => {
    fetch('/api/data/underlying/NIFTY')
        .then(res => res.json())
        .then(data => updateUI(data));
}, 1000);
```

**Problems:**
1. **Latency**: Average 500ms delay (poll interval / 2)
2. **Bandwidth waste**: 1 request/sec even if no new data
3. **Server load**: 1000 clients = 1000 req/sec
4. **Battery drain**: Mobile devices constantly requesting

**WebSocket (Our approach):**
```javascript
const socket = io('http://localhost:8002');
socket.on('underlying_update', (data) => {
    updateUI(data);  // Instant update when data available
});
```

**Benefits:**
1. **Low latency**: <100ms (push immediately when data arrives)
2. **Efficient**: Server pushes only when there's new data
3. **Bi-directional**: Server can push, client can send
4. **Persistent connection**: One connection, many messages

**Comparison:**

**HTTP Polling:**
- Request overhead: ~500 bytes (headers) per request
- 1000 clients Ã— 1 req/sec Ã— 500 bytes = 500 KB/sec bandwidth
- Plus response data (another 1 KB) = 1.5 MB/sec total

**WebSocket:**
- Initial handshake: ~500 bytes (once)
- Message overhead: ~10 bytes per message
- 1000 clients Ã— 1 msg/sec Ã— 10 bytes = 10 KB/sec
- Plus message data (1 KB) = 1 MB/sec total

**~30% bandwidth savings**

**Why WebSocket for trading:**
- Real-time prices (latency matters)
- Frequent updates (1+ per second)
- Scalability (thousands of connections)
- User experience (live updates, no lag)

**When HTTP polling is okay:**
- Infrequent updates (weather app, hourly check)
- Simple deployment (no WebSocket infrastructure)
- Caching helps (same data for all users)

For our real-time trading platform, WebSocket is the only viable option."

**Follow-up Q: How do you monitor WebSocket connections?**

"We track connection metrics and expose them via `/metrics` endpoint:

```python
# Track connected clients
connected_clients = {}

@socketio.on('connect')
def handle_connect():
    client_id = request.sid
    connected_clients[client_id] = {
        'connected_at': time.time(),
        'rooms': ['general']
    }

@app.route('/metrics')
def metrics():
    # Total connections
    total_clients = len(connected_clients)
    
    # Connections per room
    room_counts = {}
    for client_id, client_info in connected_clients.items():
        for room in client_info['rooms']:
            room_counts[room] = room_counts.get(room, 0) + 1
    
    # Connection duration
    durations = [time.time() - c['connected_at'] for c in connected_clients.values()]
    avg_duration = sum(durations) / len(durations) if durations else 0
    
    return {
        'total_clients': total_clients,
        'rooms': room_counts,
        'avg_connection_duration_seconds': avg_duration
    }
```

**Prometheus metrics:**
```python
from prometheus_client import Gauge, Counter

websocket_connections = Gauge('websocket_connections_total', 'Total WebSocket connections')
websocket_messages_sent = Counter('websocket_messages_sent_total', 'Total messages sent')
websocket_messages_received = Counter('websocket_messages_received_total', 'Total messages received')

@socketio.on('connect')
def handle_connect():
    websocket_connections.inc()

@socketio.on('disconnect')
def handle_disconnect():
    websocket_connections.dec()

def redis_listener():
    for message in pubsub.listen():
        socketio.emit('update', data)
        websocket_messages_sent.inc()
```

**Grafana dashboard:**
- Total connections over time
- Connections per room (which products are popular)
- Messages/sec throughput
- Connection churn rate (connects - disconnects)

**Alerting:**
```python
if total_clients > 5000:
    send_alert("WebSocket connections > 5000, consider scaling")

if avg_connection_duration < 60:
    send_alert("High churn rate, investigate connection stability")
```"

---

## ðŸ“š **SECTION 6: KUBERNETES & DEPLOYMENT**

---

### **Q14: Explain your Kubernetes deployment. How does HPA work?**

**Answer:**
"We deploy the platform on Kubernetes with Horizontal Pod Autoscaler for dynamic scaling.

**Deployment manifest:**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worker-enricher
spec:
  replicas: 5  # Default replicas
  selector:
    matchLabels:
      app: worker-enricher
  template:
    spec:
      containers:
      - name: worker-enricher
        image: option-aro/worker-enricher:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

**HPA Configuration:**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: worker-enricher-hpa
spec:
  scaleTargetRef:
    kind: Deployment
    name: worker-enricher
  minReplicas: 2   # Never fewer than 2
  maxReplicas: 10  # Never more than 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 70  # Target 70% CPU
  - type: Resource
    resource:
      name: memory
      target:
        averageUtilization: 80  # Target 80% memory
```

**How HPA works:**

**1. Monitoring:**
- HPA queries metrics-server every 15 seconds
- Gets current CPU/memory usage per pod

**2. Decision logic:**
```
desired_replicas = current_replicas Ã— (current_metric / target_metric)

Example:
current_replicas = 5
current_cpu = 85%
target_cpu = 70%

desired = 5 Ã— (85 / 70) = 6.07 â†’ Scale to 6 replicas
```

**3. Scaling actions:**
```
Current: 2 replicas, CPU at 80%
â†“ HPA scales up
Now: 3 replicas, CPU at 60% (below 70% target)
â†“ Stable for 5 minutes
â†“ Load decreases, CPU at 40%
â†“ HPA scales down
Now: 2 replicas, CPU at 60%
```

**4. Cooldown:**
- Scale-up: Immediate (handle load spikes)
- Scale-down: 5 min cooldown (avoid thrashing)

**Real-world scenario:**

**Market opens (9:15 AM):**
```
8:00 AM: 2 replicas, low volume
9:15 AM: Market opens, volume spikes
9:16 AM: CPU hits 90%, HPA scales to 5 replicas
9:17 AM: CPU drops to 65%, stable
3:30 PM: Market closes, volume drops
3:35 PM: CPU at 30%, HPA scales down to 3 replicas
3:40 PM: Still low, scales down to 2 replicas
```

**Benefits:**
- **Cost efficiency**: Pay only for what you need
- **Automatic**: No manual scaling
- **Responsive**: Handles unexpected load
- **Resilience**: If pod crashes, HPA maintains desired count

**Other services:**
- API Gateway: 2-5 replicas (lower variability)
- Socket Gateway: 3-8 replicas (connection-dependent)
- Workers: 2-10 replicas (highest variability)"

**Follow-up Q: What if scaling isn't fast enough for sudden spikes?**

"Good question. HPA reactive scaling has a lag:
1. Metrics collected every 15s
2. Decision made
3. Pod creation takes 30-60s (image pull, container start)
4. Pod registers with service

Total: ~90 seconds to scale up

**Solutions:**

**1. Vertical Pod Autoscaler (VPA):**
```yaml
# Increase resource limits, not replicas
requests:
  cpu: "250m" â†’ "500m"  # Give more CPU to existing pods
```

**2. Pre-warming:**
```yaml
minReplicas: 5  # Start with higher baseline
# Accept slightly higher cost for faster response
```

**3. Predictive scaling (custom metrics):**
```python
# Scale based on queue depth, not CPU
from kubernetes import client, config

queue_depth = redis_client.llen('celery')
if queue_depth > 100:
    # Scale up proactively
    scale_deployment('worker-enricher', replicas=8)
```

**4. Cluster Autoscaler:**
```
HPA scales pods: 5 â†’ 10 replicas
â†“ Not enough nodes to schedule 10 pods
â†“ Cluster Autoscaler adds nodes
â†“ Pods scheduled on new nodes
```

**5. Lower CPU threshold:**
```yaml
target:
  averageUtilization: 50  # Scale earlier (more headroom)
```

**6. Scheduled scaling (cron):**
```python
# Scale up before market opens
# 8:45 AM: Scale to 8 replicas (pre-warm for 9:15 AM open)
# 4:00 PM: Scale down to 2 replicas (post market close)
```

**For production trading system, I'd use:**
- Pre-warmed baseline (5 replicas minimum during market hours)
- Queue-depth based custom metrics (more responsive than CPU)
- Scheduled scaling (predictable daily patterns)
- Lower CPU threshold (60% instead of 70%)"

---

## ðŸ“š **SECTION 7: OBSERVABILITY & MONITORING**

---

### **Q15: How do you debug issues in a distributed system with 8 services?**

**Answer:**
"Debugging distributed systems is challenging because a request spans multiple services. We use structured logging, trace IDs, and centralized metrics.

**Problem: User reports 'NIFTY option chain not loading'**

**Step 1: Check API Gateway logs**
```bash
docker logs api-gateway | grep NIFTY
```

**Without structured logging (hard to parse):**
```
ERROR - Chain request failed for NIFTY 2025-01-25
```

**With structured logging (easy to query):**
```json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "error",
  "service": "api-gateway",
  "endpoint": "/api/data/chain/NIFTY",
  "product": "NIFTY",
  "expiry": "2025-01-25",
  "error": "Storage service unavailable",
  "status_code": 503,
  "trace_id": "abc-123-def"
}
```

**Now I know:**
- Which service failed: Storage
- Error type: Service unavailable
- Trace ID to follow request through system

**Step 2: Check Storage Service logs (using trace_id)**
```bash
docker logs storage | grep abc-123-def
```

```json
{
  "timestamp": "2025-01-15T10:30:00.123Z",
  "level": "error",
  "service": "storage",
  "operation": "get_option_chain",
  "product": "NIFTY",
  "error": "MongoDB connection timeout",
  "trace_id": "abc-123-def"
}
```

**Root cause: MongoDB timeout**

**Step 3: Check MongoDB**
```bash
docker logs mongodb

# See high connection count
connections: 95/100 (near limit)
```

**Step 4: Check Worker logs (they might be overwhelming MongoDB)**
```bash
docker logs worker-enricher

# See thousands of option chains being processed
processed_option_chain: 1523 chains in last minute
```

**Root cause identified:**
- Workers processing too many chains
- MongoDB connection pool exhausted
- Storage service can't get connections

**Solution:**
1. Increase MongoDB connection pool: 100 â†’ 200
2. Rate-limit worker processing
3. Add MongoDB read replica for load distribution

**Our structured logging implementation:**
```python
import structlog
import uuid

# Add trace ID to context
def add_trace_id(logger, method_name, event_dict):
    if 'trace_id' not in event_dict:
        event_dict['trace_id'] = str(uuid.uuid4())
    return event_dict

structlog.configure(
    processors=[
        add_trace_id,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()

# Usage
logger.error(
    "chain_request_failed",
    product="NIFTY",
    expiry="2025-01-25",
    error=str(e),
    trace_id=request_id
)
```

**Propagating trace ID across services:**
```python
# API Gateway forwards trace ID
response = requests.get(
    f"{STORAGE_SERVICE_URL}/option/chain/{product}",
    headers={'X-Trace-ID': trace_id}
)

# Storage Service extracts trace ID
trace_id = request.headers.get('X-Trace-ID', str(uuid.uuid4()))
logger.info("processing_request", trace_id=trace_id)
```

**Centralized logging (production):**
```
All services â†’ Fluentd/Filebeat â†’ Elasticsearch â†’ Kibana
                                     (searchable)  (visualize)

Query: trace_id:"abc-123-def"
Result: All logs for that request across all services
```

This approach reduces debugging time from hours to minutes."

**Follow-up Q: What metrics do you monitor?**

"We monitor four categories:

**1. Service Health (Uptime, errors)**
```python
@app.route('/health')
def health():
    return {'status': 'healthy', 'service': 'api-gateway'}

# Prometheus scrapes /health every 15s
# Alert if service down for >1 minute
```

**2. Performance (Latency, throughput)**
```python
from prometheus_client import Histogram, Counter

request_latency = Histogram('request_duration_seconds', 'Request latency')
request_count = Counter('requests_total', 'Total requests', ['endpoint', 'status'])

@app.route('/api/data/chain/<product>')
@request_latency.time()
def get_chain(product):
    result = process_request()
    request_count.labels(endpoint='/api/data/chain', status='200').inc()
    return result

# Alert if p95 latency > 500ms
# Alert if throughput < 100 req/min (traffic drop)
```

**3. Business Metrics (Application-specific)**
```python
# Trading-specific metrics
chains_processed = Counter('option_chains_processed_total', 'Chains processed')
pcr_gauge = Gauge('latest_pcr', 'Latest PCR value', ['product'])

def process_option_chain(data):
    chains_processed.inc()
    pcr_gauge.labels(product=data['product']).set(data['pcr'])

# Alert if chains_processed = 0 for 5 min (feed generator down)
# Alert if PCR > 2.0 (unusual market sentiment)
```

**4. Infrastructure (CPU, memory, connections)**
```python
# Kubernetes metrics (via metrics-server)
- Pod CPU usage
- Pod memory usage
- Node capacity

# Custom metrics
websocket_connections = Gauge('websocket_connections', 'Active WebSocket connections')
celery_queue_depth = Gauge('celery_queue_depth', 'Tasks in Celery queue')

# Alert if memory > 80%
# Alert if CPU > 90% for >5 min
# Alert if queue depth > 1000 (workers can't keep up)
```

**Grafana Dashboard:**
```
Row 1: Service Health (API Gateway, Storage, Workers)
Row 2: Request Latency (p50, p95, p99)
Row 3: Throughput (Requests/sec, Chains/sec)
Row 4: Business Metrics (PCR, Active products, Subscriptions)
Row 5: Infrastructure (CPU, Memory, Network)
```

**Alerting rules:**
```yaml
# Prometheus alert rules
groups:
  - name: option-aro-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        annotations:
          summary: "High error rate: {{ $value }}%"
      
      - alert: HighLatency
        expr: histogram_quantile(0.95, request_duration_seconds) > 0.5
        for: 5m
        annotations:
          summary: "P95 latency > 500ms"
```"

---

This guide covers the core technical concepts and interview questions. Would you like me to continue with more sections on specific topics like:
- Option Greeks calculations
- MongoDB indexing strategies  
- Docker best practices
- System design questions
- Behavioral questions about your project?
